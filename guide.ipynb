{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5362b89d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **Gu√≠a - Reinforcement Learning**\n",
    "- Elaborado por: Miguel P√©rez\n",
    "\n",
    "---\n",
    "# Tipos de Datos\n",
    "\n",
    "### **Datos Discretos (enteros)**\n",
    "- Valores contables  \n",
    "- Ej: edad, n√∫mero de fallas\n",
    "\n",
    "### **Datos Categ√≥ricos**\n",
    "- **Binomiales:** 2 categor√≠as (s√≠/no, hombre/mujer)  \n",
    "- **Multinomiales:** >2 categor√≠as (tipo de combustible, color de ojos)\n",
    "\n",
    "### **Datos Continuos**\n",
    "- Valores en un intervalo infinito  \n",
    "- Ej: temperatura, humedad, velocidad, voltaje\n",
    "\n",
    "---\n",
    "\n",
    "## Histogramas\n",
    "- Representan **frecuencias** de valores o rangos.  \n",
    "- Sirve para:\n",
    "  - Entender distribuci√≥n  \n",
    "  - Ver patrones  \n",
    "  - Detectar valores comunes o raros  \n",
    "\n",
    "---\n",
    "\n",
    "# Distribuciones\n",
    "\n",
    "### **Distribuci√≥n Uniforme**\n",
    "- Todos los valores tienen la MISMA probabilidad.  \n",
    "- Ej: dado justo, baraja mezclada.\n",
    "\n",
    "### **Distribuci√≥n Binomial**\n",
    "- Solo dos resultados posibles: √©xito / fracaso.  \n",
    "- Ej: lanzar una moneda n veces.\n",
    "\n",
    "### **Distribuci√≥n Multinomial**\n",
    "- M√°s de dos categor√≠as excluyentes.  \n",
    "- Ej: salidas de las caras de un dado.\n",
    "\n",
    "### **Distribuci√≥n Normal (Gaussiana)**\n",
    "- Forma de campana.  \n",
    "- Media = mediana = moda  \n",
    "- Com√∫n en fen√≥menos naturales.\n",
    "\n",
    "### **Distribuci√≥n Poisson**\n",
    "- Cuenta eventos en un intervalo.  \n",
    "- Ej: llamadas por minuto, gotas por segundo.\n",
    "\n",
    "### **Distribuci√≥n Pareto**\n",
    "- Describe fen√≥menos 80-20.  \n",
    "- Ej: riqueza, producci√≥n, ventas.\n",
    "\n",
    "### **Distribuci√≥n Beta**\n",
    "- Variable continua entre 0 y 1.  \n",
    "- Modela proporciones.\n",
    "\n",
    "---\n",
    "\n",
    "# Medidas Estad√≠sticas\n",
    "\n",
    "### **Moda**\n",
    "Valor que m√°s se repite.\n",
    "\n",
    "### **Media**\n",
    "$$\n",
    "\\text{mean} = \\frac{\\sum x}{n}\n",
    "$$\n",
    "\n",
    "### **Mediana**\n",
    "Valor central del conjunto ordenado.\n",
    "\n",
    "### **Varianza**\n",
    "Como estan dispersos los datos en una distribuci√≥n.\n",
    "$$\n",
    "\\sigma^2 = \\frac{\\sum (x-\\mu)^2}{n}\n",
    "$$\n",
    "\n",
    "### **Desviaci√≥n Est√°ndar**\n",
    "Indica los rangos en los que los datos se presentan en mayor proporci√≥n.\n",
    "$$\n",
    "\\sigma = \\sqrt{\\text{Var}}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Teorema Central del L√≠mite (CLT)\n",
    "\n",
    "- Si tomas **muchas muestras** de cualquier distribuci√≥n,\n",
    "  la **distribuci√≥n de sus medias ser√° Normal**.\n",
    "- Usado en:\n",
    "  - Ciencia\n",
    "  - Encuestas\n",
    "  - ML: Policy Gradients, m√©todos con expected value\n",
    "\n",
    "---\n",
    "\n",
    "# Probabilidad\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{\\text{veces que ocurre}}{\\text{total de eventos}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# PDF ‚Äî Probability Density Function\n",
    "\n",
    "Each distribution type has a function called the Probability Density Function (PDF) which intends to model the density of a given dataset and return a number between 0 and 1 that signals how dense the data is. Each distribution has its own PDF equation.\n",
    "\n",
    "## **PDF Gaussiana / Discretos**\n",
    "$$\n",
    "\\mu = mean\n",
    "$$\n",
    "$$\n",
    "\\sigma = Std Dev\n",
    "$$\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "## **PDF Binomial / Categ√≥rica**\n",
    "\n",
    "$$\n",
    "\\beta = Spread (modifies\\;sigmoid-like \\;curve)\n",
    "$$\n",
    "$$a_1, a_2, a_N = parameters$$\n",
    "\n",
    "$$\n",
    "P(a_1, a_2, \\beta) =  \\sigma(\\beta(a_1-a_2)) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(a_1, a_2,...,a_N) =  \\frac{e^{ a_i }}{\\sum e^{ a_j }}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# CDF ‚Äî Cumulative Distribution Function\n",
    "\n",
    "Da la probabilidad de que:  \n",
    "$$\n",
    "X \\leq x\n",
    "$$\n",
    "\n",
    "## Outliers (IQR)\n",
    "$$\n",
    "\\text{IQR} = Q3 - Q1\n",
    "$$\n",
    "\n",
    "Outlier si:\n",
    "$$\n",
    "x > Q3 + 1.5(IQR)\n",
    "$$\n",
    "o  \n",
    "$$\n",
    "x < Q1 - 1.5(IQR)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Valor Esperado (Expected Value)\n",
    "\n",
    "## Sin pesos:\n",
    "$$\n",
    "E[X] = \\frac{1}{N}\\sum x_i\n",
    "$$\n",
    "\n",
    "## Con pesos/p(x):\n",
    "Average of data transformed by a function (e.g.\n",
    "log(x)) weighed by its likelihood (i.e. p(x))\n",
    "$$\n",
    "E[f(X)] = \\sum p(x_i)\\, f(x_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Multi-Armed Bandit (Tragamonedas)\n",
    "\n",
    "Actualizaci√≥n incremental del valor esperado:\n",
    "\n",
    "$$\n",
    "Q_k = Q_{k-1} + \\frac{1}{k}(r_k - Q_{k-1})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# Exploration vs Exploitation\n",
    "\n",
    "## **Epsilon-Greedy**\n",
    "$$\n",
    "\\epsilon_t = \\epsilon_{end} + (\\epsilon_{start}-\\epsilon_{end}) e^{-t/\\text{decay}}\n",
    "$$\n",
    "\n",
    "- random > Œµ ‚Üí explotaci√≥n  \n",
    "- random ‚â§ Œµ ‚Üí exploraci√≥n  \n",
    "\n",
    "## **Softmax (Boltzmann) o Sigmoid para muestreo**\n",
    "$$\n",
    "P(a) = \\frac{e^{\\beta Q(a)}}{\\sum e^{\\beta Q(a')}}\n",
    "$$\n",
    "$$\n",
    "P(Q, \\beta) =  \\sigma(\\beta(Q_a-Q_b)) = \\frac{1}{1+e^{-\\beta (Q_a-Q_b)}}\n",
    "$$\n",
    "Œ≤ par√°metro de control (‚Äútemperature‚Äù) controla qu√© tan ‚Äúdeterminista‚Äù es la elecci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "# Reglas de Probabilidad\n",
    "\n",
    "## **Suma ( two mutually exclusive events happening):**\n",
    "$$\n",
    "P(A \\cup B) = P(A) + P(B)\n",
    "$$\n",
    "\n",
    "## **Producto ( both events happening ):**\n",
    "$$\n",
    "P(A \\cap B) = P(A)P(B)\n",
    "$$\n",
    "\n",
    "## **Probabilidad Condicional**\n",
    "Compute the probability of A\n",
    "given the ocurrence of B. This means that\n",
    "B must happen first, subject to its own\n",
    "uncertainty, and only then, from what is\n",
    "left, A can happen with a given\n",
    "probability\n",
    "$$\n",
    "P(A|B) = \\frac{P(A\\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Base del Teorema de Bayes.\n",
    "\n",
    "---\n",
    "\n",
    "#  Maximum Likelihood ‚Äî Apuntes\n",
    "\n",
    "## Probabilidad vs Likelihood\n",
    "- **Probabilidad**: dado un modelo (par√°metros conocidos), ¬øqu√© tan probable es que X tome ciertos valores?  \n",
    "- **Likelihood (verosimilitud)**: dada una observaci√≥n $x$ y una familia de distribuciones parametrizadas por $\\theta$, la verosimilitud es:\n",
    "\n",
    "$$\n",
    "L(\\theta) = f(x \\mid \\theta)\n",
    "$$\n",
    "\n",
    "## PDF normal (Gaussiana)\n",
    "$$\n",
    "f(x \\mid \\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "## Maximum Likelihood\n",
    "Para datos $x_1,\\dots,x_n$ y par√°metros $\\theta$:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^n f(x_i \\mid \\theta), \\qquad\n",
    "\\ln L(\\theta) = \\sum_{i=1}^n \\ln f(x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "Use logarithms to simplify computations and make use of its concave property\n",
    "\n",
    "## Distribuci√≥n Normal donde $\\theta=[\\mu,\\sigma^2]$\n",
    "\n",
    "Log-likelihood:\n",
    "$$\n",
    "\\ln L(\\mu,\\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "Derivando parcialmente e igualando a cero:\n",
    "\n",
    "- Estimador batch de la media (MLE):\n",
    "  $$\n",
    "  \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n x_i\n",
    "  $$\n",
    "\n",
    "- Estimador batch de la varianza (MLE):\n",
    "  $$\n",
    "  \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\hat{\\mu})^2\n",
    "  $$\n",
    "\n",
    "## Optimizaci√≥n por gradiente\n",
    "- Calcula $\\nabla_\\theta \\ln L(\\theta)$ y usa descenso por gradiente:\n",
    "\n",
    "  $$\n",
    "  \\theta \\leftarrow \\theta + \\eta \\nabla_\\theta \\ln L(\\theta)\n",
    "  $$\n",
    "\n",
    "  (o la variante con signo negativo si minimizas la *neg-log-likelihood*).\n",
    "\n",
    "## Estimaci√≥n secuencial (online)\n",
    "- Media incremental:\n",
    "  $$\n",
    "  \\hat{\\mu}_{t+1} = \\hat{\\mu}_t + \\frac{1}{N}(x_{t+1} - \\hat{\\mu}_t)\n",
    "    $$\n",
    "- Varianza incremental (forma simple mostrada):\n",
    "  $$\n",
    "  \\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{(x_{t+1} - \\hat{\\mu}_t)^2 - \\hat{\\sigma}^2_t}{t+1}\n",
    "  $$\n",
    "  $$\n",
    "  \\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{1}{N}((x_t - \\hat{\\mu}) - \\hat{\\sigma}^2_t)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Ejemplo: Actualizaci√≥n de par√°metros con Policy Gradient (Gaussian PDF, Online)\n",
    "\n",
    "### Condiciones iniciales\n",
    "\n",
    "- Distribuci√≥n de acciones: $a \\sim \\mathcal{N}(\\mu, \\sigma^2)$  \n",
    "- Par√°metros iniciales: \n",
    "$$\n",
    "\\mu_0 = 0, \\quad \\sigma_0^{2} = 1\n",
    "$$\n",
    "- Learning rate impl√≠cito mediante promedio online: $\\frac{1}{N}$  \n",
    "- Recompensas asumidas $r_t = 1$  \n",
    "- Acciones tomadas por iteraci√≥n: $a = [0.5, 0.6, 0.4, 0.7, 0.3]$\n",
    "\n",
    "---\n",
    "\n",
    "### F√≥rmulas de actualizaci√≥n online\n",
    "\n",
    "- Media incremental:\n",
    "$$\n",
    "\\hat{\\mu}_{t+1} = \\hat{\\mu}_t + \\frac{1}{t+1} \\left(a_{t+1} - \\hat{\\mu}_t\\right)\n",
    "$$\n",
    "\n",
    "- Varianza incremental:\n",
    "$$\n",
    "\\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{(a_{t+1} - \\hat{\\mu}_t)^2 - \\hat{\\sigma}^2_t}{t+1}\n",
    "$$\n",
    "\n",
    "> Nota: Esta es la forma **online**, equivalente a un promedio ponderado secuencial que se puede usar en Policy Gradient.\n",
    "\n",
    "---\n",
    "\n",
    "### Iteraci√≥n 0\n",
    "\n",
    "- Acci√≥n: $a_1 = 0.5$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_1 = 0 + \\frac{0.5 - 0}{1} = 0.5\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_1^2 = 1 + \\frac{(0.5 - 0)^2 - 1}{1} = 0.25\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteraci√≥n 1\n",
    "\n",
    "- Acci√≥n: $a_2 = 0.6$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_2 = 0.5 + \\frac{0.6 - 0.5}{2} = 0.55\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_2^2 = 0.25 + \\frac{(0.6 - 0.5)^2 - 0.25}{2} = 0.125\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteraci√≥n 2\n",
    "\n",
    "- Acci√≥n: $a_3 = 0.4$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_3 = 0.55 + \\frac{0.4 - 0.55}{3} \\approx 0.5\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_3^2 = 0.125 + \\frac{(0.4 - 0.55)^2 - 0.125}{3} \\approx 0.0833\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteraci√≥n 3\n",
    "\n",
    "- Acci√≥n: $a_4 = 0.7$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_4 = 0.5 + \\frac{0.7 - 0.5}{4} = 0.55\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_4^2 = 0.0833 + \\frac{(0.7 - 0.5)^2 - 0.0833}{4} \\approx 0.0917\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteraci√≥n 4\n",
    "\n",
    "- Acci√≥n: $a_5 = 0.3$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_5 = 0.55 + \\frac{0.3 - 0.55}{5} = 0.5\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_5^2 = 0.0917 + \\frac{(0.3 - 0.55)^2 - 0.0917}{5} \\approx 0.0834\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen de iteraciones\n",
    "\n",
    "| Iteraci√≥n | Acci√≥n $a_t$ | $\\mu_t$ | $\\sigma^2_t$ |\n",
    "|-----------|--------------|---------|-------------|\n",
    "| 0         | 0.5          | 0.5     | 0.25        |\n",
    "| 1         | 0.6          | 0.55    | 0.125       |\n",
    "| 2         | 0.4          | 0.5     | 0.0833      |\n",
    "| 3         | 0.7          | 0.55    | 0.0917      |\n",
    "| 4         | 0.3          | 0.5     | 0.0834      |\n",
    "\n",
    "- La **media $\\mu$** se ajusta hacia las acciones tomadas con mayor recompensa.  \n",
    "- La **varianza $\\sigma^2$** refleja la dispersi√≥n de las acciones y se ajusta secuencialmente en cada iteraci√≥n.  \n",
    "- Este enfoque es equivalente a una actualizaci√≥n online de Policy Gradient para distribuciones gaussianas.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Multinomial Distribution Approximation - Policy Gradient - Categorical (softmax)\n",
    "## Multinomial PDF\n",
    "\n",
    "Sea un conjunto de par√°metros $a_k$, donde $a_j$ es el par√°metro asociado a la categor√≠a cuya probabilidad queremos modelar.\n",
    "\n",
    "La probabilidad de la categor√≠a $j$ est√° dada por:\n",
    "\n",
    "$$\n",
    "S_j = \\frac{e^{\\beta a_j}}{\\sum_{i=1}^{n} e^{\\beta a_i}}\n",
    "$$\n",
    "\n",
    "donde $\\beta$ es un par√°metro de control (similar al \"inverse temperature\").\n",
    "\n",
    "---\n",
    "\n",
    "## Optimization (Maximum Likelihood)\n",
    "\n",
    "Queremos ajustar los par√°metros $\\{a_1, a_2, ..., a_k, \\beta\\}$ para aproximar correctamente la distribuci√≥n multinomial.\n",
    "\n",
    "La PDF general:\n",
    "\n",
    "$$\n",
    "\\text{pdf}(x \\mid \\theta) = \\frac{e^{\\beta a_x}}{\\sum_{i=1}^{n} e^{\\beta a_i}}\n",
    "$$\n",
    "\n",
    "La likelihood del dataset:\n",
    "\n",
    "$$\n",
    "L(x_1,\\dots,x_n \\mid \\theta) = \\prod_{i=1}^{n} \\text{pdf}(x_i\\mid\\theta)\n",
    "$$\n",
    "\n",
    "### Derivadas parciales (softmax)\n",
    "\n",
    "Derivada del score $S_j$ respecto a su par√°metro:\n",
    "\n",
    "- **Cuando $i = j$**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_j}{\\partial a_j} = S_j(1 - S_j)\n",
    "$$\n",
    "\n",
    "- **Cuando $i \\neq j$**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S_j}{\\partial a_i} = -S_j S_i\n",
    "$$\n",
    "\n",
    "Estas son exactamente las derivadas del **softmax** est√°ndar.\n",
    "\n",
    "---\n",
    "\n",
    "# Optimization via Policy Learning\n",
    "\n",
    "En un agente RL, si cada acci√≥n corresponde a una categor√≠a de la distribuci√≥n multinomial, la pol√≠tica viene dada por:\n",
    "\n",
    "$$\n",
    "p(a_i) = \\frac{e^{\\beta a_i}}{\\sum_{k=1}^n e^{\\beta a_k}}\n",
    "$$\n",
    "\n",
    "El **valor esperado del retorno** es:\n",
    "\n",
    "$$\n",
    "\\bar{r} = \\sum_{i=1}^n p(a_i)\\, r_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient of Expected Reward\n",
    "\n",
    "El gradiente del valor esperado respecto al par√°metro $a_i$ es:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\bar{r}}{\\partial a_i}\n",
    "= \\beta\\, p(a_i)\\,\\cdot r\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Parameter update rules\n",
    "\n",
    "### Regla de actualizaci√≥n general:\n",
    "\n",
    "$$\n",
    "a_i \\leftarrow a_i + \\lambda\\, \\frac{\\partial \\bar{r}}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "Sustituyendo el gradiente:\n",
    "\n",
    "$$\n",
    "a_i \\leftarrow a_i + \\lambda \\beta\\, \\cdot r\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "## Policy Gradient (Softmax)\n",
    "\n",
    "Actualizaci√≥n de par√°metros para una pol√≠tica estoc√°stica parametrizada por logits $Œ∏$ y softmax.  \n",
    "\n",
    "### Softmax & Probabilidades\n",
    "Dada la pol√≠tica:\n",
    "\n",
    "$$\n",
    "\\text{probs} = \\text{softmax}(Œ∏)\n",
    "$$\n",
    "\n",
    "### Gradiente online para acci√≥n tomada\n",
    "Si se toma la acci√≥n $i$ con recompensa $r$:\n",
    "\n",
    "$$\n",
    "\\text{grad} = 0 \\quad \\text{y solo grad}[i] = 1\n",
    "$$\n",
    "\n",
    "La actualizaci√≥n de par√°metros es:\n",
    "\n",
    "$$\n",
    "Œ∏ \\leftarrow Œ∏ + \\alpha \\cdot (\\text{grad} - \\text{probs}) \\cdot r\n",
    "$$\n",
    "\n",
    "### Interpretaci√≥n\n",
    "- La acci√≥n tomada se **refuerza**: $Œ∏_i$ sube si $r$ es positivo.  \n",
    "- Las otras acciones se **debilitan** proporcionalmente a su probabilidad.  \n",
    "- Equivalente a la derivada del softmax *policy gradient*:  \n",
    "\n",
    "Para acci√≥n tomada:\n",
    "\n",
    "$$\n",
    "Œ∏_i \\leftarrow Œ∏_i + \\lambda (1 - p(a_i)) \\, \\cdot r\n",
    "$$\n",
    "\n",
    "Para acciones no tomadas ($j \\neq i$):\n",
    "\n",
    "$$\n",
    "Œ∏_j \\leftarrow Œ∏_j - \\lambda \\, p(a_j) \\,  \\cdot r\n",
    "$$\n",
    "---\n",
    "\n",
    "## Ejemplo: Actualizaci√≥n de par√°metros softmax (Policy Gradient online)\n",
    "\n",
    "Condiciones iniciales:\n",
    "\n",
    "- Par√°metros iniciales: \n",
    "$$\n",
    "\\theta = [0, 0]\n",
    "$$\n",
    "- Softmax inicial: \n",
    "$$\n",
    "\\text{softmax}(\\theta) = [0.5, 0.5]\n",
    "$$\n",
    "- Learning rate: $\\alpha = 0.1$  \n",
    "- Recompensa: $r = 1$  \n",
    "- Acci√≥n seleccionada: √≠ndice 0  \n",
    "\n",
    "F√≥rmula de actualizaci√≥n:\n",
    "\n",
    "$$\n",
    "\\theta \\gets \\theta + \\alpha \\cdot (\\text{grad} - \\text{probs}) \\cdot r\n",
    "$$\n",
    "\n",
    "donde para la acci√≥n seleccionada:  \n",
    "$$\n",
    "\\text{grad} = [1,0]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteraciones (resumen)\n",
    "\n",
    "| Iteraci√≥n | $\\theta_0$ | $\\theta_1$ | Softmax $\\text{probs}$ |\n",
    "|-----------|------------|------------|-----------------------|\n",
    "| 0         | 0          | 0          | [0.5, 0.5]           |\n",
    "| 1         | 0.05       | -0.05      | [0.512, 0.488]       |\n",
    "| 2         | 0.0975     | -0.0975    | [0.524, 0.476]       |\n",
    "| 3         | 0.1426     | -0.1426    | [0.537, 0.463]       |\n",
    "| 4         | 0.1891     | -0.1891    | [0.549, 0.451]       |\n",
    "| 5         | 0.2344     | -0.2344    | [0.562, 0.438]       |\n",
    "\n",
    "- Los par√°metros de la **acci√≥n elegida aumentan**, los de la **no elegida disminuyen**.  \n",
    "- Las probabilidades softmax favorecen progresivamente la acci√≥n seleccionada.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# RL Cheat Sheet: Tipos de Acci√≥n y Algoritmos\n",
    "\n",
    "| Tipo de Acci√≥n / PDF        | Algoritmo                  | Actualizaci√≥n Online / F√≥rmulas |\n",
    "|------------------------------|---------------------------|--------------------------------|\n",
    "| **Discreta (pocas acciones)** | Q-Learning (tabular / Deep Q) | **Simple:**<br>$$Q_k = Q_{k-1} + \\alpha (r_k - Q_{k-1})$$<br>**Bellman / TD:**<br>$$Q(s,a) \\gets Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$ |\n",
    "| **Discreta (pol√≠tica suave)** | Policy Gradient (Softmax) | **Acci√≥n tomada $i$:**<br>$$\\theta_i \\gets \\theta_i + \\alpha (1 - p_i)(r - b)$$<br>**Acciones no tomadas $j\\neq i$:**<br>$$\\theta_j \\gets \\theta_j - \\alpha p_j (r - b)$$ |\n",
    "| **Continua (acciones reales)** | Policy Gradient (Gaussian / REINFORCE) | **Media incremental:**<br>$$\\hat{\\mu}_{t+1} = \\hat{\\mu}_t + \\frac{1}{N}(x_{t+1} - \\hat{\\mu}_t)$$<br>**Varianza incremental:**<br>$$\\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{(x_{t+1} - \\hat{\\mu}_t)^2 - \\hat{\\sigma}^2_t}{t+1}$$<br>o alternativamente:<br>$$\\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{1}{N}((x_t - \\hat{\\mu}) - \\hat{\\sigma}^2_t)$$ |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Concepto | F√≥rmula / Descripci√≥n |\n",
    "|----------|--------------------|\n",
    "| **Media secuencial** | $$\\hat{\\mu}_{t+1} = \\hat{\\mu}_t + \\frac{(x_{t+1} - \\hat{\\mu}_t)}{N}$$ |\n",
    "| **Media normal (MLE)** | $$\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{N} \\sum_{i=1}^{N} x_i$$ |\n",
    "| **Varianza secuencial** | $$\\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{(x_t - \\hat{\\mu})^2 - \\hat{\\sigma}^2_t}{N}$$ |\n",
    "| **Varianza normal (MLE)** | $$\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{\\mu})^2$$ |\n",
    "| **Softmax** | $$\\pi(a=j \\mid s) = \\frac{e^{\\beta a_j}}{\\sum_{i=1}^{k} e^{\\beta a_i}}$$ |\n",
    "| **Derivada softmax (acci√≥n elegida)** | $$\\theta_i \\gets \\theta_i + \\alpha (1 - p_i)(r - b)$$ $$\\frac{\\partial S_j}{\\partial a_j} = S_j(1 - S_j)$$|\n",
    "| **Derivada softmax (acci√≥n no elegida)** | $$\\theta_j \\gets \\theta_j - \\alpha p_j (r - b)$$ $$\\frac{\\partial S_j}{\\partial a_i} = -S_j S_i$$ |\n",
    "| **Actualizaci√≥n par√°metros (datos discretos)** | $$a_j \\gets a_j + \\alpha (1 - s_j) R$$ <br> $$a_i \\gets a_i + \\alpha (0 - s_i) R, \\quad i \\neq j$$ <br> $s_j$ = probabilidad actual de la acci√≥n $j$ |\n",
    "| **Q-Learning** | $$Q(s,a) \\gets Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$ $\\gamma$: factor de descuento ‚Üí cu√°nto valen las recompensas futuras. <br> - Si $\\gamma \\approx 1$: agente paciente, valora recompensas futuras. <br> - Si $\\gamma$ es bajo: agente solo considera recompensas inmediatas. |\n",
    "| **Decaimiento exponencial de Œµ** | $$\\text{eps\\_threshold} = \\text{eps\\_end} + (\\text{eps\\_start} - \\text{eps\\_end}) \\cdot \\exp(- \\text{steps} / \\text{eps\\_decay})$$ <br> - Con probabilidad $1-\\epsilon$: explota ‚Üí elige acci√≥n con mayor $Q$. <br> - Con probabilidad $\\epsilon$: explora ‚Üí elige acci√≥n aleatoria. |\n",
    "| **PDF de distribuci√≥n normal** | $$f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{- \\frac{(x - \\mu)^2}{2\\sigma^2}}$$ |\n",
    "| **Valor esperado** | $$E[f(X)] = \\sum p(x_i)\\, f(x_i)$$ $$Q_k = Q_{k-1} + \\frac{1}{k}(r_k - Q_{k-1})$$ |\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Reinforcement Learning (RL)\n",
    "\n",
    "---\n",
    "\n",
    "## Introducci√≥n y Fundamentos\n",
    "\n",
    "### Machine Learning: Comparativa\n",
    "* **Supervised Learning:** Tenemos datos etiquetados por humanos (Input $\\to$ Target).\n",
    "* **Unsupervised Learning:** Tenemos datos, pero no etiquetas (buscamos patrones/estructuras).\n",
    "* **Reinforcement Learning:** **No tenemos datos previos**. Tenemos un **agente** y un **entorno** que provee **recompensas**.\n",
    "\n",
    "### Elementos del RL\n",
    "El ciclo b√°sico de interacci√≥n:\n",
    "1.  **Agente:** Entidad artificial que analiza observaciones y emite acciones.\n",
    "2.  **Entorno (Environment):** Sistema que recibe la acci√≥n, transiciona a un nuevo estado y emite una observaci√≥n y una recompensa.\n",
    "3.  **Recompensa (Reward):** Se√±al escalar que indica qu√© tan buena fue la acci√≥n con respecto a un objetivo.\n",
    "4.  **Pol√≠tica (Policy):** La estrategia del agente (mapeo de observaciones a acciones).\n",
    "\n",
    "> **El Problema Central:** C√≥mo observar, recolectar y analizar datos para emitir acciones que **maximicen la recompensa acumulada**.\n",
    "\n",
    "### Tipos de Motivaci√≥n\n",
    "* **Extr√≠nseca:** La recompensa es dise√±ada por humanos (ingenier√≠a de recompensas) para guiar al agente (ej. puntos en un juego).\n",
    "* **Intr√≠nseca:** Se√±al generada por el propio agente para fomentar la exploraci√≥n.\n",
    "    * *Curiosidad:* Basada en el error de predicci√≥n (si no puedo predecir qu√© pasar√°, quiero ir ah√≠).\n",
    "    * *Empowerment:* Capacidad de controlar el entorno.\n",
    "\n",
    "---\n",
    "\n",
    "## Tipos de Enfoques en RL\n",
    "\n",
    "### Por Modelo\n",
    "1.  **Model-Free (Libre de modelo):** Mapea observaciones directamente a acciones o valores usando prueba y error. No intenta entender \"c√≥mo funciona la f√≠sica\" del entorno.\n",
    "2.  **Model-Based (Basado en modelo):**\n",
    "    * Entrena un modelo para predecir la din√°mica del entorno (Estado actual + Acci√≥n $\\to$ Siguiente Estado).\n",
    "    * Usa ese modelo para planificar o entrenar una pol√≠tica.\n",
    "\n",
    "### Por M√©todo de Aprendizaje\n",
    "1.  **Value-based:** Aprende el valor num√©rico de estar en un estado o tomar una acci√≥n ($Q$). Elige la acci√≥n con mayor valor.\n",
    "2.  **Policy-based:** Aprende directamente la funci√≥n de probabilidad de las acciones dado un estado.\n",
    "3.  **Actor-Critic:** H√≠brido. Un *Actor* decide la acci√≥n y un *Cr√≠tico* estima el valor de esa acci√≥n para ajustar al actor.\n",
    "\n",
    "---\n",
    "\n",
    "## Entornos y Gymnasium\n",
    "\n",
    "### Tipos de Entornos\n",
    "* **K-armed Bandits:** Tragamonedas. Elegir opciones con diferentes probabilidades de recompensa (sin estados secuenciales).\n",
    "* **Mazes (Laberintos):** Espacio navegable con obst√°culos y metas.\n",
    "* **Robots:** Sistemas mec√°nicos (caminar, agarrar). Control motor continuo.\n",
    "* **Juegos:** StarCraft, Atari. Usados para testear algoritmos (benchmarks).\n",
    "\n",
    "### Estructura Gymnasium (Python Wrapper)\n",
    "Librer√≠a est√°ndar para entornos de RL. Clase principal `CustomEnv`:\n",
    "* `__init__()`: Constructor, define variables iniciales.\n",
    "* `reset()`: Restaura el entorno al inicio y devuelve la primera observaci√≥n.\n",
    "* `step(action)`: Ejecuta una acci√≥n. Retorna:\n",
    "    * `observation`: Nuevo estado.\n",
    "    * `reward`: Recompensa obtenida.\n",
    "    * `terminated/truncated`: Booleano (¬øtermin√≥ el juego?).\n",
    "\n",
    "---\n",
    "\n",
    "## Value-Based Methods (M√©todos Basados en Valor)\n",
    "\n",
    "### Concepto Biol√≥gico\n",
    "Inspirado en la dopamina. Las neuronas refuerzan sinapsis cuando la recompensa recibida es mayor a la esperada (error de predicci√≥n positivo).\n",
    "\n",
    "### Expected Value (Valor Esperado)\n",
    "Es el promedio ponderado de los resultados posibles.\n",
    "$$E[f] = \\sum p(x_i) f(x_i)$$\n",
    "* Donde $p(x_i)$ es la probabilidad de que ocurra el evento y $f(x_i)$ el valor del evento.\n",
    "\n",
    "### Q-Learning (Tabular)\n",
    "El cerebro/agente modela el valor esperado de las opciones.\n",
    "\n",
    "**Actualizaci√≥n de Valor (Simple):**\n",
    "$$Q_k = Q_{k-1} + \\alpha (r_k - Q_{k-1})$$\n",
    "* $Q_k$: Valor acumulado.\n",
    "* $\\alpha$ (o $1/k$): Tasa de aprendizaje (Learning Rate).\n",
    "* $r_k$: Recompensa actual.\n",
    "* **Interpretaci√≥n:** El nuevo valor es el viejo valor m√°s una fracci√≥n del \"error\" (diferencia entre lo que recib√≠ y lo que cre√≠a que iba a recibir).\n",
    "\n",
    "**Valor Relativo (Bavard et al.):**\n",
    "El cerebro normaliza los valores bas√°ndose en el contexto (min y max recompensas disponibles).\n",
    "$$Q_k = Q_{k-1} + \\alpha \\left( \\frac{r_{obj} - r_{min}}{r_{max} - r_{min}} - Q_{k-1} \\right)$$\n",
    "\n",
    "### Temporal Difference (TD) y Ecuaci√≥n de Bellman\n",
    "Para decisiones secuenciales (donde el futuro importa).\n",
    "**Ecuaci√≥n Clave:**\n",
    "$$Q(s, a)_{new} = Q(s, a)_{old} + \\alpha \\underbrace{[r + \\gamma \\cdot \\max Q(s', a') - Q(s, a)_{old}]}_{\\text{TD Error}}$$\n",
    "\n",
    "* $\\gamma$ (Gamma): Factor de descuento. Qu√© tanto me importa el futuro vs el presente.\n",
    "* $\\max Q(s', a')$: La mejor suposici√≥n del valor del *siguiente* estado.\n",
    "\n",
    "---\n",
    "\n",
    "#### Ejemplo: Actualizaci√≥n de Q-Learning (Tabular)\n",
    "\n",
    "##### Condiciones iniciales\n",
    "\n",
    "- Estado √∫nico: $s$  \n",
    "- Acciones posibles: $A = \\{a_0, a_1\\}$  \n",
    "- Valores iniciales: \n",
    "$$\n",
    "Q(s, a_0) = 0, \\quad Q(s, a_1) = 0\n",
    "$$\n",
    "- Learning rate: $\\alpha = 0.1$  \n",
    "- Recompensas recibidas por acci√≥n: $r = [1, 0.5, 1, 0, 0.7]$  \n",
    "- No se considera futuro (TD simple, $\\gamma = 0$)  \n",
    "\n",
    "##### F√≥rmula de actualizaci√≥n simple (Tabular Q-Learning)\n",
    "\n",
    "$$\n",
    "Q_k = Q_{k-1} + \\alpha \\cdot (r_k - Q_{k-1})\n",
    "$$\n",
    "\n",
    "> Nota: El nuevo Q es el viejo Q m√°s una fracci√≥n del error (diferencia entre recompensa recibida y la predicci√≥n previa).\n",
    "\n",
    "---\n",
    "\n",
    "##### Iteraciones\n",
    "\n",
    "| Iteraci√≥n | Acci√≥n tomada | $Q_{prev}$ | Recompensa $r$ | $Q_{new}$ |\n",
    "|-----------|---------------|------------|----------------|-----------|\n",
    "| 0         | $a_0$         | 0          | 1              | $0 + 0.1*(1-0) = 0.1$ |\n",
    "| 1         | $a_1$         | 0          | 0.5            | $0 + 0.1*(0.5-0) = 0.05$ |\n",
    "| 2         | $a_0$         | 0.1        | 1              | $0.1 + 0.1*(1-0.1) = 0.19$ |\n",
    "| 3         | $a_1$         | 0.05       | 0              | $0.05 + 0.1*(0-0.05) = 0.045$ |\n",
    "| 4         | $a_0$         | 0.19       | 0.7            | $0.19 + 0.1*(0.7-0.19) = 0.241$ |\n",
    "\n",
    "---\n",
    "\n",
    "##### Resumen\n",
    "\n",
    "- Los valores $Q(s, a)$ se **acercan progresivamente a la recompensa promedio** de cada acci√≥n.  \n",
    "- La acci√≥n $a_0$ tiene un Q mayor porque hist√≥ricamente dio m√°s recompensa.  \n",
    "- La **f√≥rmula es muy intuitiva:** ajusta el valor anterior usando el error entre lo que se recibi√≥ y lo esperado.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Markov Decision Process\n",
    "\n",
    "Un MDP es un **caso especial de las Cadenas de Markov**.\n",
    "* **Cadena de Markov normal:** Las transiciones ocurren de forma estoc√°stica \"porque s√≠\" (fen√≥menos naturales).\n",
    "* **MDP:** Las transiciones son provocadas por una **fuente externa** (Agente o Usuario). El sistema no cambia de estado a menos que se ejecute una acci√≥n ($a$).\n",
    "\n",
    "### Ciclo de Interacci√≥n\n",
    "1.  **Estado Actual:** El sistema est√° en un estado $S$.\n",
    "2.  **Acci√≥n Externa:** El agente selecciona una acci√≥n ($a_{ij}$) de una matriz de acciones posibles.\n",
    "3.  **Transici√≥n y Recompensa:** El sistema cambia al siguiente estado y \"devuelve\" una recompensa ($r_{ij}$).\n",
    "\n",
    "### La Propiedad de Markov (The Markovian Assumption)\n",
    "Es la regla de oro para que las matem√°ticas funcionen. Establece que el futuro es independiente del pasado, dado el presente.\n",
    "> *\"La probabilidad de pasar al siguiente estado y obtener una recompensa depende **√∫nicamente** del estado actual y la acci√≥n tomada, no de la historia previa.\"*\n",
    "\n",
    "$$P(S_{t+1} | S_t, a_t, S_{t-1}, ...) = P(S_{t+1} | S_t, a_t)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Exploration vs Exploitation\n",
    "\n",
    "El dilema: ¬øPruebo algo nuevo (aprender) o elijo lo que s√© que funciona (ganar recompensa)?\n",
    "\n",
    "1.  **Epsilon-Greedy ($\\epsilon$-greedy):**\n",
    "    * Tirar un dado. Si sale bajo ($\\epsilon$), elijo una acci√≥n aleatoria (Exploraci√≥n).\n",
    "    * Si sale alto, elijo la mejor acci√≥n conocida (Explotaci√≥n).\n",
    "    * *Decay:* $\\epsilon$ empieza alto (mucho random) y baja con el tiempo.\n",
    "\n",
    "2.  **Softmax / Sigmoide:**\n",
    "    * Convierte los valores $Q$ en probabilidades. Si una acci√≥n es mucho mejor, tiene mucha m√°s probabilidad de ser elegida, pero no el 100%.\n",
    "    * $$P(a) = \\frac{e^{Q(a)/\\tau}}{\\sum e^{Q(b)/\\tau}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Policy Gradients (Gradientes de Pol√≠tica)\n",
    "\n",
    "### Objetivo\n",
    "Optimizar directamente los par√°metros ($\\theta$) de la pol√≠tica $\\pi$ para maximizar la recompensa total esperada ($J$).\n",
    "\n",
    "Parametrizamos la pol√≠tica como $\\pi_\\theta(a|s)$ y queremos **maximizar** el retorno esperado sobre trayectorias $\\tau$:\n",
    "\n",
    "$$\n",
    "J(\\theta)=\\mathbb{E}_{\\tau\\sim p_\\theta(\\tau)}\\big[ R(\\tau) \\big]\n",
    "$$\n",
    "\n",
    "donde $R(\\tau)=\\sum_{t=0}^{T} r_t$. Aqu√≠ $p_\\theta(\\tau)$ es la probabilidad de la trayectoria bajo la pol√≠tica y la din√°mica del entorno.\n",
    "\n",
    "## 2) Log-derivative trick\n",
    "\n",
    "Queremos $\\nabla_\\theta J(\\theta)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta)\n",
    "&= \\nabla_\\theta \\int p_\\theta(\\tau)\\, R(\\tau)\\, d\\tau \\\\\n",
    "&= \\int \\nabla_\\theta p_\\theta(\\tau)\\, R(\\tau)\\, d\\tau\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Usamos el truco de derivada logar√≠tmica:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta p_\\theta(\\tau)=p_\\theta(\\tau)\\nabla_\\theta \\log p_\\theta(\\tau)\n",
    "$$\n",
    "\n",
    "Sustituyendo:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)=\\mathbb{E}_{\\tau\\sim p_\\theta}\\big[ \\nabla_\\theta \\log p_\\theta(\\tau)\\, R(\\tau) \\big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Factorizaci√≥n por pasos de tiempo\n",
    "\n",
    "La probabilidad de una trayectoria:\n",
    "\n",
    "$$\n",
    "p_\\theta(\\tau)=p(s_0)\\prod_{t=0}^{T} \\pi_\\theta(a_t|s_t)\\, p(s_{t+1}|s_t,a_t)\n",
    "$$\n",
    "\n",
    "Tomando logaritmo:\n",
    "\n",
    "$$\n",
    "\\log p_\\theta(\\tau)=\\sum_{t=0}^{T} \\log \\pi_\\theta(a_t|s_t) + \\text{const}\n",
    "$$\n",
    "\n",
    "Por tanto, la Ecuaci√≥n del Gradiente:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta)=\n",
    "\\mathbb{E}_{\\tau\\sim p_\\theta}\n",
    "\\left[\n",
    "\\sum_{t=0}^{T}\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\,\\cdot R(\\tau)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "* **Interpretaci√≥n:** Ajustamos los par√°metros $\\theta$ para hacer m√°s probables las acciones $(a_t)$ que resultaron en una alta recompensa acumulada $R(\\tau)$.\n",
    "\n",
    "## Problemas Comunes en Policy Gradient\n",
    "### 1. **Alta Varianza en el Gradiente**\n",
    "\n",
    "Los m√©todos de Policy Gradient estiman el gradiente esperado de la recompensa:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s)\\, R \\right]\n",
    "$$\n",
    "\n",
    "El problema es que la estimaci√≥n:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_i|s_i) R_i\n",
    "$$\n",
    "\n",
    "tiene **varianza muy alta**, especialmente cuando:\n",
    "\n",
    "- $R$ depende de trayectorias largas\n",
    "- el espacio de estados es grande\n",
    "- las pol√≠ticas cambian demasiado entre actualizaciones\n",
    "\n",
    "Esto provoca **inestabilidad**, actualizaciones ruidosas y aprendizaje lento.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Exploraci√≥n Ineficiente**\n",
    "\n",
    "La pol√≠tica se actualiza en la direcci√≥n de acciones que han dado buena recompensa:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a|s) R\n",
    "$$\n",
    "\n",
    "Si la pol√≠tica inicial es mala y produce pocas acciones con recompensa:\n",
    "\n",
    "- el gradiente es peque√±o\n",
    "- la pol√≠tica no explora lo suficiente\n",
    "- se queda atrapada en √≥ptimos locales\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Sensibilidad a Hiperpar√°metros**\n",
    "\n",
    "Especialmente al *learning rate*:\n",
    "\n",
    "- si es muy peque√±o ‚Üí aprendizaje extremadamente lento  \n",
    "- si es muy grande ‚Üí divergencia\n",
    "\n",
    "---\n",
    "\n",
    "### Baseline: Resolver Problema de Varianza\n",
    "Los gradientes puros son muy ruidosos.\n",
    "* **Baseline:** Restar un valor base para reducir varianza de obtenci√≥n de reward y evitar sobrepremitar una acci√≥n ineficiente. \n",
    "* **Advantage Function ($A$):**\n",
    "    $$A(s, a) = Q(s, a) - V(s)$$\n",
    "    * ¬øQu√© tanto mejor es esta acci√≥n comparada con el promedio de estar en este estado?\n",
    "\n",
    "  \n",
    "Existen tres variantes cl√°sicas:\n",
    "\n",
    "\n",
    "#### 1. Baseline Global: **Promedio de Rewards Obtenidos**\n",
    "\n",
    "Se utiliza un valor escalar $b$ que promedia todos los rewards obtenidos en episodios recientes:\n",
    "\n",
    "$$\n",
    "b = \\frac{1}{N}\\sum_{i=1}^N R_i\n",
    "$$\n",
    "\n",
    "Gradiente actualizado:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J = \\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s)(R - b) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "#### 2. Baseline por Acci√≥n: **Promedio de Reward por Acci√≥n (Q-value)**\n",
    "\n",
    "Aqu√≠ se usa como baseline el valor esperado de tomar una acci√≥n en un estado:\n",
    "\n",
    "$$\n",
    "b(s,a) = Q(s,a)\n",
    "$$\n",
    "\n",
    "El gradiente se vuelve:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J = \\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s)(R - Q(s,a)) \\right]\n",
    "$$\n",
    "\n",
    "Este baseline corresponde directamente a la idea de **estimadores del Q-value**.\n",
    "\n",
    "#### 3. Advantage Baseline: **$Q(s,a)$ Menos Promedio Global de Rewards**\n",
    "\n",
    "Esta variante mezcla las dos anteriores:\n",
    "\n",
    "$$\n",
    "b = \\mathbb{E}[R], \\qquad A(s,a) = Q(s,a) - b\n",
    "$$\n",
    "\n",
    "Entonces el gradiente utiliza:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J = \\mathbb{E}\\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s)\\, A(s,a) \\right]\n",
    "$$\n",
    "\n",
    "#### **Tabla Comparativa**\n",
    "\n",
    "| Variante | F√≥rmula | Ventajas | Desventajas |\n",
    "|---------|---------|----------|-------------|\n",
    "| **Promedio global de rewards** | $R - b$ | F√°cil, reduce varianza | Ignora acci√≥n/estado |\n",
    "| **Promedio por acci√≥n (Q-value)** | $R - Q(s,a)$ | Modela calidad real de acciones | Costoso; depende del critic |\n",
    "| **Q-value ‚àí promedio global** | $Q(s,a) - b$ | Combina ambas ventajas | Requiere estimar $Q$ correctamente |\n",
    "\n",
    "---\n",
    "\n",
    "## Actor-Critic y Algoritmos de Optimizaci√≥n Avanzados\n",
    "\n",
    "Cuando entrenas una pol√≠tica estoc√°stica, las acciones se eligen al azar seg√∫n sus probabilidades.\n",
    "A veces, por pura suerte, una acci√≥n mala puede recibir muchas recompensas positivas en una trayectoria espec√≠fica.\n",
    "\n",
    "¬øConsecuencia?\n",
    "El algoritmo de policy gradient ajusta la pol√≠tica para favorecer m√°s esa acci√≥n mala, porque observa que ‚Äúdio buen reward‚Äù, aunque en realidad no sea buena.\n",
    "\n",
    "Esto genera que la pol√≠tica nueva cambie demasiado respecto a la anterior, inclin√°ndose hacia acciones que aparentemente funcionaron, pero que no son realmente las mejores.\n",
    "\n",
    "### Actor-Critic\n",
    "\n",
    "El m√©todo **Actor-Critic** combina dos ideas clave:\n",
    "\n",
    "1. **Actor ($\\pi_\\theta$):** Red neuronal que decide qu√© acci√≥n tomar. (Pol√≠tica) \n",
    "2. **Critic ($V_\\phi$):** Red neuronal que estima el valor del estado ($V(s)$) para calcular el *Advantage* y reducir la varianza del gradiente.\n",
    "\n",
    "Juntos permiten entrenar pol√≠ticas estoc√°sticas m√°s estables que el m√©todo REINFORCE tradicional.\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øPor qu√© Actor-Critic?\n",
    "\n",
    "El problema del Policy Gradient puro (REINFORCE) es la **alta varianza del t√©rmino $R(\\tau)$**, lo que causa actualizaciones ruidosas y aprendizaje lento.\n",
    "\n",
    "Para solucionar esto:\n",
    "\n",
    "- Se introduce un **Critic** que estima el valor esperado del estado $V(s)$.\n",
    "- Este valor sirve como baseline para calcular el *Advantage*:\n",
    "  \n",
    "$$\n",
    "A(s,a) = R(\\tau) - V_\\phi(s)\n",
    "$$\n",
    "\n",
    "Esto estabiliza el gradiente y acelera la convergencia.\n",
    "\n",
    "---\n",
    "\n",
    "### Entrenamiento del Critic (Estimador de $V_\\phi$)\n",
    "\n",
    "El Critic se entrena para aproximar la funci√≥n de valor mediante regresi√≥n:\n",
    "\n",
    "$$\n",
    "V_\\phi(s) \\approx \\mathbb{E}[R(\\tau) \\mid s]\n",
    "$$\n",
    "\n",
    "La actualizaci√≥n del Critic es:\n",
    "\n",
    "$$\n",
    "\\phi = \\phi + \\nabla_\\phi \\left( \\| \\hat{V}_\\phi(s) - R(\\tau) \\|_2^2 \\right)\n",
    "$$\n",
    "\n",
    "Es decir:\n",
    "\n",
    "- Se minimiza el **error cuadr√°tico** entre la predicci√≥n del Critic y la recompensa real.\n",
    "- Esto convierte al Critic en un baseline adaptativo que aprende con la experiencia.\n",
    "\n",
    "---\n",
    "\n",
    "### Entrenamiento del Actor (Actualizaci√≥n de Pol√≠tica)\n",
    "\n",
    "El Actor ajusta los par√°metros $\\theta$ siguiendo un gradiente de pol√≠tica ponderado por el *Advantage* estimado:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta + \\nabla_\\theta \\log \\pi_\\theta(a \\mid s)\\, \\left( R(\\tau) - V_\\phi(s) \\right)\n",
    "$$\n",
    "\n",
    "Interpretaci√≥n:\n",
    "\n",
    "- Si una acci√≥n $a$ produjo un reward **mayor** que lo que esperaba el Critic ‚Üí la pol√≠tica debe aumentar su probabilidad.\n",
    "- Si produjo un reward **peor** de lo esperado ‚Üí la pol√≠tica debe disminuir su probabilidad.\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øQu√© est√° pasando matem√°ticamente?\n",
    "\n",
    "**Critic:** intenta responder  \n",
    "> ‚Äú¬øQu√© tan bueno es este estado en general?‚Äù\n",
    "\n",
    "**Actor:** intenta responder  \n",
    "> ‚Äú¬øDeber√≠a repetir esta acci√≥n en estados similares?‚Äù\n",
    "\n",
    "El *Advantage* los conecta:\n",
    "\n",
    "$$\n",
    "A(s,a) = R(\\tau) - V_\\phi(s)\n",
    "$$\n",
    "\n",
    "Con esto:\n",
    "\n",
    "- **El Critic reduce la varianza** del gradiente.\n",
    "- **El Actor recibe un gradiente m√°s preciso y con menos ruido.**\n",
    "\n",
    "Actor‚ÄìCritic NO controla cu√°nto cambia la pol√≠tica entre updates.\n",
    "\n",
    "El problema descrito originalmente es:\n",
    "\n",
    "- La pol√≠tica nueva puede ser demasiado diferente a la vieja.\n",
    "\n",
    "- Actor-Critic no tiene un mecanismo para limitar la magnitud del cambio de pol√≠tica.\n",
    "\n",
    "---\n",
    "### Trust Region & PPO\n",
    "\n",
    "#### Problema principal\n",
    "En *Policy Gradient* tradicional, el update sobre los par√°metros $\\theta$ puede ser tan grande que:\n",
    "- la nueva pol√≠tica se aleja demasiado de la anterior,\n",
    "- el agente ‚Äúolvida‚Äù comportamientos √∫tiles,\n",
    "- el entrenamiento se vuelve inestable y puede **divergir**.\n",
    "\n",
    "Para evitarlo nacen los **M√©todos de Optimizaci√≥n por Regiones de Confianza** (Trust Region Methods):\n",
    "\n",
    "### **TRPO ‚Äî Trust Region Policy Optimization**\n",
    "\n",
    "TRPO define una regi√≥n matem√°tica en el espacio de soluciones dentro de la cual los par√°metros pueden moverse sin destruir la pol√≠tica previa.\n",
    "\n",
    "#### üîπ Idea central  \n",
    "Limitar cu√°nto puede cambiar la pol√≠tica nueva respecto a la pol√≠tica vieja:\n",
    "- usando una restricci√≥n de **KL Divergence**,\n",
    "- para evitar saltos demasiado grandes.\n",
    "\n",
    "La actualizaci√≥n maximiza un objetivo nuevo, **pero bajo la restricci√≥n**:\n",
    "\n",
    "$$\n",
    "D_{KL}(\\pi_{\\theta_{\\text{old}}} \\;\\|\\; \\pi_{\\theta_{\\text{new}}}) \\le \\delta\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $D_{KL}$ mide cu√°nta informaci√≥n cambia entre pol√≠ticas,\n",
    "- $\\delta$ es un l√≠mite m√°ximo permitido.\n",
    "\n",
    "#### üîπ Nuevo objetivo optimizado\n",
    "TRPO maximiza una versi√≥n corregida del *surrogate objective*:\n",
    "\n",
    "$$\n",
    "L^{TRPO}(\\theta) = \n",
    "\\mathbb{E}\\left[\n",
    "\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} A^{\\pi_{\\text{old}}}(s,a)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "pero √∫nicamente permite updates $\\theta$ tal que la divergencia sea peque√±a.\n",
    "\n",
    "\n",
    "#### Ventajas de TRPO\n",
    "- Evita que la pol√≠tica cambie demasiado r√°pido.  \n",
    "- Mantiene un comportamiento m√°s estable que Policy Gradient vanilla.  \n",
    "- Reduce el riesgo de colapsar la pol√≠tica hacia acciones malas.  \n",
    "\n",
    "#### Desventaja\n",
    "- **Computacionalmente costoso:**  \n",
    "  Requiere resolver un problema de optimizaci√≥n con restricciones (m√©todo conjugado, hessianos aproximados, etc.).\n",
    "\n",
    "Esto llev√≥ a desarrollar un m√©todo m√°s simple‚Ä¶\n",
    "\n",
    "### **PPO ‚Äî Proximal Policy Optimization**\n",
    "\n",
    "PPO es una versi√≥n pr√°ctica de TRPO: mantiene la idea de limitar cu√°nto puede cambiar la pol√≠tica, pero evita la optimizaci√≥n costosa basada en KL Divergence. En su lugar, usa un mecanismo simple de **clipping** para restringir el tama√±o del update.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Idea clave del m√©todo\n",
    "\n",
    "Se calcula el **ratio** entre la pol√≠tica nueva y la antigua:\n",
    "\n",
    "$$\n",
    "r(\\theta) = \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}\n",
    "$$\n",
    "\n",
    "PPO fuerza este ratio a permanecer cerca de 1.  \n",
    "El l√≠mite se define con un hiperpar√°metro $\\epsilon$ (t√≠picamente $0.2$) y el objetivo que se maximiza es:\n",
    "\n",
    "$$\n",
    "L^{PPO}(\\theta) =\n",
    "\\mathbb{E}\\left[\n",
    "\\min\\left(\n",
    "r(\\theta)A,\\;\n",
    "\\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)A\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $r(\\theta) = \\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}$\n",
    "- $A$ es el Advantage estimado para esa muestra.\n",
    "##### üîπ 1. Cuando el ratio est√° dentro del rango permitido\n",
    "\n",
    "Si:\n",
    "\n",
    "$$\n",
    "1 - \\epsilon \\le r(\\theta) \\le 1 + \\epsilon\n",
    "$$\n",
    "\n",
    "entonces el gradiente es el mismo que en Policy Gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L =\n",
    "A \\, r(\\theta)\\, \\nabla_\\theta \\log \\pi_\\theta(a|s)\n",
    "$$\n",
    "\n",
    "que proviene de:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta r(\\theta)\n",
    "= r(\\theta)\\, \\nabla_\\theta \\log \\pi_\\theta(a|s)\n",
    "$$\n",
    "\n",
    "Este caso permite actualizar la pol√≠tica normalmente.\n",
    "\n",
    "---\n",
    "\n",
    "##### üîπ 2. Cuando el ratio se sale del rango (clipping activado)\n",
    "\n",
    "Si:\n",
    "\n",
    "$$\n",
    "r(\\theta) < 1 - \\epsilon \n",
    "\\quad \\text{o} \\quad \n",
    "r(\\theta) > 1 + \\epsilon\n",
    "$$\n",
    "\n",
    "el objetivo usa la versi√≥n recortada:\n",
    "\n",
    "$$\n",
    "\\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon) A\n",
    "$$\n",
    "\n",
    "Esta expresi√≥n es **constante respecto a** $\\theta$, por lo que:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L = 0\n",
    "$$\n",
    "\n",
    "‚û° No se actualiza la pol√≠tica para esta transici√≥n.  \n",
    "‚û° Se evita que el paso de actualizaci√≥n sea demasiado grande.\n",
    "\n",
    "\n",
    "De esta forma, PPO evita que la pol√≠tica cambie demasiado r√°pido y se vuelva inestable.\n",
    "\n",
    "---\n",
    "\n",
    "#### Actualizaci√≥n Actor‚ÄìCritic en PPO\n",
    "\n",
    "PPO sigue el esquema Actor‚ÄìCritic:\n",
    "\n",
    "- **Critic ($V_\\phi$)** estima $V(s)$ para construir la funci√≥n Advantage:  \n",
    "  $$A = R(\\tau) - V_\\phi(s)$$\n",
    "\n",
    "- **Actor ($\\pi_\\theta$)** actualiza sus par√°metros usando el objetivo clipped.\n",
    "\n",
    "El gradiente real se calcula como:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta) =\n",
    "\\mathbb{E}_\\tau\n",
    "\\left[\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot A_t\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "pero modulado por el clipping, es decir, s√≥lo se propaga si el ratio est√° dentro del rango permitido.\n",
    "\n",
    "Esto produce:\n",
    "- actualizaci√≥n para la acci√≥n elegida:  \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta)\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t=0}^{T}\n",
    "\\frac{s_i (1 - s_i)}{sOld_j} \\cdot\n",
    "\\text{Adv}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "- actualizaci√≥n ligera o nula para las acciones no elegidas:  \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta)\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t=0}^{T}\n",
    "\\frac{s_i (s_j)}{sOld_j} \\cdot\n",
    "\\text{Adv}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "seg√∫n si fueron penalizadas por el clipping.\n",
    "\n",
    "En PPO, la actualizaci√≥n de par√°metros depende de c√≥mo cambian las probabilidades de la pol√≠tica nueva respecto a la antigua: \n",
    "\n",
    "> Para la acci√≥n elegida, el gradiente es fuerte: aumenta o disminuye su probabilidad seg√∫n el Advantage, y se escala por el ratio entre pol√≠tica nueva y vieja. Esto refuerza buenas acciones y penaliza malas, pero solo dentro de un l√≠mite seguro determinado por el clipping.\n",
    "\n",
    "> Para las acciones no elegidas, la actualizaci√≥n es indirecta y mucho m√°s peque√±a: se ajustan sus probabilidades para mantener una distribuci√≥n coherente, pero sin alterar dr√°sticamente la pol√≠tica. Si el ratio sale del rango permitido, PPO aplica clipping y ambos tipos de actualizaciones se reducen o eliminan, evitando cambios bruscos o inestables.\n",
    "\n",
    "**En esencia: la acci√≥n elegida recibe el update principal, las no elegidas solo peque√±os ajustes, y PPO asegura que nada cambie demasiado r√°pido.**\n",
    "\n",
    "---\n",
    "\n",
    "#### Ventajas de PPO\n",
    "- Reduce varianza y evita actualizaciones peligrosas.\n",
    "- Mantiene estabilidad similar a TRPO sin su costo computacional.\n",
    "- Permite m√∫ltiples pasos de optimizaci√≥n por cada batch (a diferencia de PG cl√°sico).\n",
    "- Es actualmente uno de los m√©todos est√°ndar en entornos de RL modernos.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparaci√≥n PPO y TRPO\n",
    "| M√©todo | Estabilidad | Coste computacional | Control del cambio |\n",
    "|--------|-------------|---------------------|--------------------|\n",
    "| Policy Gradient | Baja | Bajo | Ninguno |\n",
    "| **TRPO** | Muy alta | **Muy alto** | KL Divergence estricta |\n",
    "| **PPO** | Alta | Bajo/Medio | Ratio con clipping (trust region suave) |\n",
    "\n",
    "---\n",
    "\n",
    "## **Deep Q-Networks (DQN)**\n",
    "\n",
    "Cuando el espacio de estados es demasiado grande para usar una tabla Q cl√°sica (como en videojuegos tipo Atari donde los estados son **im√°genes**), se utiliza una **Red Neuronal Convolucional (CNN)** para aproximar la funci√≥n de acci√≥n-valor:\n",
    "\n",
    "$$\n",
    "Q_\\theta(s, a)\n",
    "$$\n",
    "\n",
    "### **Idea General**\n",
    "- **Input:** Una imagen o stack de im√°genes (estado).\n",
    "- **Output:** Un vector de valores Q, uno por cada acci√≥n posible.\n",
    "- **Aprendizaje:** La red no aprende a ‚Äúclasificar‚Äù, sino a detectar **caracter√≠sticas visuales que indican valor futuro**.\n",
    "\n",
    "\n",
    "\n",
    "## **¬øQu√© aprende realmente un DQN?**\n",
    "Una CNN de visi√≥n tradicional aprende *features* para reconocer objetos.  \n",
    "Un **DQN** aprende *features* que le dicen:\n",
    "\n",
    "> \"Si est√°s viendo este patr√≥n visual, esta acci√≥n futura tiende a generar buena recompensa.\"\n",
    "\n",
    "Es decir, aprende a ver la pantalla como un humano experto: detecta **se√±ales √∫tiles para sobrevivir, esquivar, atacar, etc.**\n",
    "\n",
    "\n",
    "\n",
    "### **El Ciclo Completo de Aprendizaje de un DQN**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Observaci√≥n del Estado**\n",
    "\n",
    "El agente recibe el estado actual:\n",
    "\n",
    "$$ s_t $$\n",
    "\n",
    "que suele ser una imagen o un stack de varios frames (para capturar movimiento).\n",
    "\n",
    "La red neuronal (CNN + capas densas) produce un **vector de valores Q** para cada acci√≥n:\n",
    "\n",
    "$$\n",
    "Q_\\theta(s_t, a_1),\\;\n",
    "Q_\\theta(s_t, a_2),\\;\n",
    "\\dots,\\;\n",
    "Q_\\theta(s_t, a_n)\n",
    "$$\n",
    "\n",
    "Cada valor representa la estimaci√≥n de cu√°n buena es cada acci√≥n desde ese estado.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Selecci√≥n de Acci√≥n (Exploraci√≥n vs. Explotaci√≥n)**\n",
    "\n",
    "El agente decide la acci√≥n mediante una pol√≠tica **$\\varepsilon$-greedy**:\n",
    "\n",
    "- Con prob. $\\varepsilon$: toma una **acci√≥n aleatoria** (explora).  \n",
    "- Con prob. $1 - \\varepsilon$: toma la **mejor acci√≥n seg√∫n la red**.\n",
    "\n",
    "Formalmente:\n",
    "\n",
    "$$\n",
    "a_t = \n",
    "\\begin{cases}\n",
    "\\text{acci√≥n aleatoria}, & \\text{si } \\text{Uniform}(0,1) < \\varepsilon \\\\\n",
    "\\arg\\max_a Q_\\theta(s_t, a), & \\text{si } \\text{Uniform}(0,1) \\ge \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Ejecuci√≥n de la Acci√≥n**\n",
    "\n",
    "Tras ejecutar $a_t$, el entorno devuelve:\n",
    "\n",
    "- el siguiente estado $s_{t+1}$\n",
    "- la recompensa inmediata $r_t$\n",
    "- un indicador $done$ que dice si el episodio termin√≥\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Almacenamiento en Replay Buffer**\n",
    "\n",
    "Se guarda la transici√≥n completa:\n",
    "\n",
    "$$\n",
    "(s_t, a_t, r_t, s_{t+1}, done)\n",
    "$$\n",
    "\n",
    "El **Replay Buffer** permite:\n",
    "\n",
    "- romper correlaciones temporales en los datos  \n",
    "- entrenar la red con *mini-batches* independientes  \n",
    "- reutilizar experiencias muchas veces\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Muestreo de un Mini-Batch**\n",
    "\n",
    "Para entrenar, se selecciona un conjunto aleatorio de experiencias:\n",
    "\n",
    "$$\n",
    "\\{(s_i, a_i, r_i, s'_i, done_i)\\}_{i=1}^N\n",
    "$$\n",
    "\n",
    "Esto da gradientes m√°s estables que entrenar con datos consecutivos del episodio.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. C√°lculo del Target TD**\n",
    "\n",
    "El objetivo (target) para el aprendizaje proviene del m√©todo de **Temporal Difference (TD)** y se calcula usando la **Target Network** $Q_{\\theta^-}$, una copia estable de la red.\n",
    "\n",
    "- Si la transici√≥n **no es terminal**:\n",
    "\n",
    "$$\n",
    "y_i = r_i + \\gamma \\max_{a'} Q_{\\theta^-}(s'_i, a')\n",
    "$$\n",
    "\n",
    "- Si **es terminal**:\n",
    "\n",
    "$$\n",
    "y_i = r_i\n",
    "$$\n",
    "\n",
    "La red principal NUNCA se usa para el $\\max$ aqu√≠; por estabilidad, solo se usa la Target Network.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Actualizaci√≥n de la Q-Network**\n",
    "\n",
    "La red principal se entrena minimizando el error cuadr√°tico entre la predicci√≥n y el target:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \n",
    "\\frac{1}{N} \\sum_{i=1}^N \n",
    "\\left( y_i - Q_\\theta(s_i, a_i) \\right)^2\n",
    "$$\n",
    "\n",
    "Actualizaci√≥n por gradiente descendente:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Actualizaci√≥n de la Target Network**\n",
    "\n",
    "Cada cierto n√∫mero de pasos, sincronizamos las redes:\n",
    "\n",
    "- **Hard update**:\n",
    "\n",
    "$$\n",
    "\\theta^- \\leftarrow \\theta\n",
    "$$\n",
    "\n",
    "- **Soft update** (m√°s estable):\n",
    "\n",
    "$$\n",
    "\\theta^- \\leftarrow \\tau \\theta + (1 - \\tau)\\theta^-\n",
    "$$\n",
    "\n",
    "donde $0 < \\tau \\ll 1$ (ej. $10^{-3}$).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Resumen Conceptual**\n",
    "**Un DQN:**\n",
    "- Observa un estado visual.  \n",
    "- Estima $Q(s, a)$ mediante una CNN.  \n",
    "- Usa TD-learning para ajustar esos valores.  \n",
    "- Aprende a reconocer patrones visuales que indican qu√© acciones son mejor a largo plazo.  \n",
    "- Se entrena de manera estable gracias a **Replay Memory** y **Target Networks**.\n",
    "\n",
    "---\n",
    "\n",
    "## Model-Based & Advanced Architectures\n",
    "El aprendizaje por refuerzo basado en modelos entrena un modelo a partir del muestreo de la din√°mica del entorno y entrena su pol√≠tica a partir del muestreo de este modelo. A continuaci√≥n se presentan estos modelos:\n",
    "\n",
    "---\n",
    "\n",
    "### **World Models**\n",
    "\n",
    "Los *World Models* buscan que el agente no solo reaccione al entorno, sino que **aprenda su propia simulaci√≥n interna del mundo**. La gracia es que el agente deja de entrenar directamente sobre im√°genes crudas y empieza a entrenar en un espacio m√°s simple y estructurado. Este ‚Äúmundo interno‚Äù se construye usando tres m√≥dulos: un **VAE**, un **MDN-RNN** y un **Controller**.\n",
    "\n",
    "#### **¬øQu√© es el Espacio Latente?**\n",
    "\n",
    "Un **espacio latente** es una **versi√≥n comprimida de los datos originales** donde solo se conservan las caracter√≠sticas m√°s importantes.  \n",
    "Este espacio permite representar informaci√≥n compleja en pocas dimensiones, facilitando el an√°lisis, la predicci√≥n y la generaci√≥n de datos.\n",
    "\n",
    "En muchos modelos, este espacio se describe mediante los **par√°metros de varias distribuciones gaussianas** (medias y varianzas), lo que permite capturar estructuras complejas del mundo real en una forma compacta y manipulable.\n",
    "\n",
    "Ejemplo: Una imagen de 64√ó64√ó3 son 12‚Äâ288 valores. El VAE puede convertir eso en un vector de, digamos, 32 dimensiones.\n",
    "\n",
    "Lo importante:\n",
    "\n",
    "- No es un ‚Äúpixelado‚Äù ni un recorte; es una **codificaci√≥n abstracta** de las caracter√≠sticas relevantes.  \n",
    "- En un buen espacio latente, puntos cercanos representan estados visualmente o sem√°nticamente parecidos.  \n",
    "- Aprender pol√≠ticas en este espacio es m√°s f√°cil porque el agente trabaja con una **versi√≥n organizada** del mundo en lugar de im√°genes ruidosas y enormes.\n",
    "\n",
    "En resumen: el latente es ‚Äúlo que necesitas saber, sin la basura‚Äù.\n",
    "\n",
    "\n",
    "#### **1. VAE (Vision Module)**\n",
    "\n",
    "El **Variational Autoencoder** toma cada frame del entorno y lo comprime a un vector latente:\n",
    "\n",
    "$$ z_t = \\text{Encoder}(s_t) $$\n",
    "\n",
    "Ese vector:\n",
    "\n",
    "- contiene la informaci√≥n relevante de la imagen  \n",
    "- elimina detalles irrelevantes  \n",
    "- sigue una distribuci√≥n gaussiana aprendida\n",
    "\n",
    "La estructura general es:\n",
    "\n",
    "$$\n",
    "s_t \\rightarrow \\text{Encoder} \\rightarrow z_t\n",
    "$$\n",
    "\n",
    "y tambi√©n aprende a decodificar:\n",
    "\n",
    "$$\n",
    "z_t \\rightarrow \\text{Decoder} \\rightarrow \\hat{s}_t\n",
    "$$\n",
    "\n",
    "Esto obliga al VAE a aprender una representaci√≥n compacta y √∫til.\n",
    "\n",
    "\n",
    "#### **2. MDN-RNN (Memory Module)**\n",
    "\n",
    "Una vez que tenemos estados comprimidos $z_t$, el siguiente paso es aprender **c√≥mo evoluciona el mundo**.\n",
    "\n",
    "Para eso, se usa un **Recurrent Neural Network** (t√≠picamente un LSTM), pero no cualquiera: se convierte en un **Mixture Density Network** (MDN).  Esto significa que la red no predice un √∫nico siguiente estado, sino los **par√°metros de varias distribuciones gaussianas**:\n",
    "\n",
    "$$\n",
    "P(z_{t+1} \\mid z_t, a_t, h_t)\n",
    "$$\n",
    "\n",
    "donde $h_t$ es el hidden state del RNN.\n",
    "\n",
    "¬øPor qu√© mezclar gaussianas?\n",
    "\n",
    "- Porque el futuro no siempre es determinista.  \n",
    "- En muchos juegos, desde un mismo estado pueden ocurrir varios eventos posibles.  \n",
    "- El MDN captura esa **incertidumbre** al predecir m√∫ltiples gausianas (cada una con su media, varianza y peso).\n",
    "\n",
    "Lo que hace el MDN-RNN:\n",
    "\n",
    "- Modela las din√°micas del entorno en el espacio latente.  \n",
    "- Predice si el episodio terminar√° pronto.  \n",
    "- Mantiene una memoria $h_t$ que representa el ‚Äúestado interno‚Äù de la secuencia.\n",
    "\n",
    "En f√≥rmula simplificada:\n",
    "\n",
    "$$\n",
    "(z_t, a_t, h_t) \\rightarrow \\text{MDN-RNN} \\rightarrow (\\text{Gaussians for } z_{t+1},\\; \\text{done probability})\n",
    "$$\n",
    "\n",
    "#### **3. Controller (Policy Module)**\n",
    "\n",
    "Ahora que ya existe:\n",
    "\n",
    "- una representaci√≥n visual comprimida $z_t$\n",
    "- un modelo del futuro $h_t$\n",
    "\n",
    "solo falta un m√≥dulo que tome decisiones.\n",
    "\n",
    "El **Controller** suele ser una red neuronal muy peque√±a (incluso lineal en el paper original):\n",
    "\n",
    "$$\n",
    "a_t = C(z_t, h_t)\n",
    "$$\n",
    "\n",
    "Este m√≥dulo es el que implementa la pol√≠tica. Lo interesante:\n",
    "\n",
    "- Ya no necesita ver im√°genes crudas.  \n",
    "- Ya no necesita aprender las din√°micas del entorno.  \n",
    "- Solo aprende a actuar usando el simulador interno que construyeron el VAE y el MDN-RNN.\n",
    "\n",
    "Esto reduce brutalmente la complejidad del problema.\n",
    "\n",
    "---\n",
    "\n",
    "#### **La Gran Ventaja: El Agente Puede ‚ÄúSo√±ar‚Äù**\n",
    "\n",
    "Como el MDN-RNN puede predecir $z_{t+1}$ y el indicador de finalizaci√≥n, el agente puede:\n",
    "\n",
    "- **simular episodios completos dentro de su mente**,  \n",
    "- sin tocar el entorno real,  \n",
    "- generando millones de experiencias baratas y r√°pidas.\n",
    "\n",
    "El Controller puede entrenarse completamente dentro de esta simulaci√≥n:\n",
    "\n",
    "$$\n",
    "\\text{Controller} \\;\\text{entrena en el mundo generado por}\\; (VAE + MDN\\text{-}RNN)\n",
    "$$\n",
    "\n",
    "y luego se transfiere al entorno real.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pros y Contras**\n",
    "\n",
    "**Pros:**\n",
    "- Enorme eficiencia de muestras: la pol√≠tica se entrena en el espacio latente y en simulaciones internas.  \n",
    "- Permite entrenar m√∫ltiples tareas sobre el mismo modelo del mundo.  \n",
    "- El VAE reduce la complejidad del input visual de forma masiva.\n",
    "\n",
    "**Contras:**\n",
    "- El MDN-RNN a veces falla al modelar din√°micas dif√≠ciles.  \n",
    "- El Controller puede aprender a explotar ‚Äúbugs‚Äù del mundo simulado. Luego esas pol√≠ticas no funcionan en el entorno real.  \n",
    "- Requiere entrenar tres modelos separados (cierta complejidad).\n",
    "\n",
    "---\n",
    "\n",
    "### **Deep Planning Network (PlaNet)**\n",
    "\n",
    "PlaNet es una mejora sobre **World Models**, dise√±ada para realizar *planeaci√≥n* directamente en el **espacio latente**, sin necesidad de entrenar una pol√≠tica tradicional.\n",
    "\n",
    "#### üîπ **Idea Central**\n",
    "En lugar de predecir im√°genes o entrenar un policy network, PlaNet:\n",
    "\n",
    "1. **Aprende un modelo del mundo en espacio latente.**  \n",
    "   El modelo predice:\n",
    "   - el siguiente estado latente,\n",
    "   - la recompensa futura,\n",
    "   - y la probabilidad de terminar el episodio.\n",
    "\n",
    "2. **Simula (‚Äúrollouts‚Äù) miles de trayectorias posibles dentro del modelo**, sin usar el entorno real.\n",
    "\n",
    "3. **Elige la secuencia de acciones** que maximiza la suma de recompensas simuladas.\n",
    "\n",
    "> Es decir: *planifica*, no solo *reacciona*.\n",
    "\n",
    "#### üîπ ¬øC√≥mo funciona PlaNet?\n",
    "\n",
    "##### **1. Aprendizaje del modelo din√°mico**\n",
    "El sistema entrena un modelo en espacio latente que captura:\n",
    "- transiciones latentes:  \n",
    "  $$ z_{t+1} = f(z_t, a_t) $$\n",
    "- recompensas:  \n",
    "  $$ r_t = g(z_t, a_t) $$\n",
    "- terminaci√≥n del episodio.\n",
    "\n",
    "Este modelo NO trabaja con im√°genes directamente; utiliza una codificaci√≥n latente compacta.\n",
    "\n",
    "##### **2. Rollouts imaginados**\n",
    "Con el modelo entrenado, PlaNet **simula miles de futuros posibles**:\n",
    "\n",
    "$$\n",
    "(z_t, a_t) \\rightarrow z_{t+1} \\rightarrow z_{t+2} \\rightarrow \\dots\n",
    "$$\n",
    "\n",
    "Cada secuencia genera una recompensa acumulada:\n",
    "\n",
    "$$\n",
    "R = \\sum_{k=0}^{H} r_{t+k}\n",
    "$$\n",
    "\n",
    "donde $H$ es el horizonte de planeaci√≥n.\n",
    "\n",
    "\n",
    "##### **3. Optimizaci√≥n de la secuencia de acciones**\n",
    "PlaNet usa m√©todos como **CEM (Cross-Entropy Method)** para buscar acciones que maximicen $R$.\n",
    "\n",
    "No aprende una pol√≠tica expl√≠cita:  \n",
    "> *elige acciones optimizando directamente la recompensa predicha*.\n",
    "\n",
    "---\n",
    "#### ‚úîÔ∏è **Pros**\n",
    "- **>5000% m√°s eficiente en muestras**: casi no requiere interacci√≥n con el entorno real.\n",
    "- Planea en un espacio comprimido ‚Üí es m√°s r√°pido y estable que trabajar con im√°genes.\n",
    "- Puede reutilizar el mismo modelo para m√∫ltiples tareas (*multi-task*).\n",
    "\n",
    "#### ‚úñÔ∏è **Contras**\n",
    "- El **modelo del mundo puede fallar** al representar din√°micas complejas.\n",
    "- El agente puede aprender a **explotar errores del modelo**, generando pol√≠ticas que no funcionan en el entorno real.\n",
    "- Requiere c√≥mputo considerable para simular miles de trayectorias en cada paso.\n",
    "\n",
    "#### **Resumen en una frase**\n",
    "PlaNet no aprende una pol√≠tica:  \n",
    "> **‚ÄúImagina‚Äù miles de futuros en su espacio latente, eval√∫a sus recompensas y act√∫a siguiendo el mejor plan.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Curiosity Driven Exploration**\n",
    "\n",
    "Cuando un entorno tiene **recompensas extremadamente escasas**, el agente puede pasar miles de episodios sin recibir se√±al √∫til.  \n",
    "Para evitar que la pol√≠tica ‚Äúno aprenda nada‚Äù, se introduce un mecanismo interno de motivaci√≥n:\n",
    "\n",
    "#### **üîπ Intrinsic Curiosity Module (ICM)**\n",
    "\n",
    "El ICM genera una **recompensa intr√≠nseca (se√±al de motivaci√≥n generada internamente por el agente, no por el entorno, cuyo prop√≥sito es incentivar la exploraci√≥n)** que motiva al agente a explorar zonas donde el modelo a√∫n no sabe predecir bien.\n",
    "\n",
    "El m√≥dulo tiene **dos componentes principales**:\n",
    "\n",
    "##### **1. Inverse Model (IM)**  \n",
    "Recibe dos estados consecutivos en *feature space*:\n",
    "\n",
    "$$ \\phi(s_t),\\; \\phi(s_{t+1}) $$\n",
    "\n",
    "y predice qu√© acci√≥n los conect√≥:\n",
    "\n",
    "$$ \\hat{a}_t = IM(\\phi(s_t), \\phi(s_{t+1})) $$\n",
    "\n",
    "Su p√©rdida es una **cross-entropy**:\n",
    "\n",
    "$$\n",
    "L_I = \\text{CE}(a_t,\\; \\hat{a}_t)\n",
    "$$\n",
    "\n",
    "Sirve para aprender representaciones de estados √∫tiles y consistentes (evita triviales cambios de p√≠xel).\n",
    "\n",
    "---\n",
    "\n",
    "##### **2. Forward Model (FM)**  \n",
    "Predice el siguiente estado latente usando el estado actual y la acci√≥n real:\n",
    "\n",
    "$$\n",
    "\\hat{\\phi}(s_{t+1}) = FM(\\phi(s_t), a_t)\n",
    "$$\n",
    "\n",
    "La **intrinsic reward** proviene del error de esta predicci√≥n:\n",
    "\n",
    "$$\n",
    "r^{int}_t = \\frac{1}{2} \\left\\|\\, \\hat{\\phi}(s_{t+1}) - \\phi(s_{t+1}) \\,\\right\\|^2\n",
    "$$\n",
    "\n",
    "Zonas donde el modelo falla ‚Üí **zonas interesantes** ‚Üí el agente quiere visitarlas.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Funci√≥n de P√©rdida Total del ICM (Intrinsic Curiosity Module)**\n",
    "\n",
    "El ICM genera **recompensa intr√≠nseca** a partir de qu√© tan dif√≠cil es predecir las consecuencias de las propias acciones del agente.  \n",
    "Para lograrlo, el m√≥dulo usa dos partes:\n",
    "\n",
    "1. **Inverse Model (IM):**  \n",
    "   Aprende a predecir la acci√≥n ejecutada $a_t$ a partir del par de estados codificados:  \n",
    "   $$\n",
    "   (\\phi(s_t), \\phi(s_{t+1}))\n",
    "   $$  \n",
    "   Esto obliga al codificador $\\phi(\\cdot)$ a retener √∫nicamente *informaci√≥n controlable por el agente*.\n",
    "\n",
    "2. **Forward Model (FM):**  \n",
    "   Predice la representaci√≥n futura:  \n",
    "   $$\\hat{\\phi}(s_{t+1}) = F(\\phi(s_t), a_t)\n",
    "   $$  \n",
    "   El error de esta predicci√≥n mide qu√© tan sorprendente o novedoso es el cambio en el entorno, y se usa como **recompensa intr√≠nseca**.\n",
    "\n",
    "**P√©rdida total del ICM**\n",
    "\n",
    "El ICM entrena ambos modelos mediante la funci√≥n:\n",
    "\n",
    "$$\n",
    "L_{ICM} = (1 - \\beta) L_I + \\beta L_F\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $\\beta$ controla el equilibrio entre aprender a **predecir acciones** (IM) y **predecir el futuro** (FM).\n",
    "- $L_I$: p√©rdida del *inverse model*, generalmente entrop√≠a cruzada al predecir $a_t$.\n",
    "- $L_F$: p√©rdida del *forward model* en espacio latente:\n",
    "\n",
    "$$\n",
    "L_F = \\left\\| \\hat{\\phi}(s_{t+1}) - \\phi(s_{t+1}) \\right\\|^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Recompensa intr√≠nseca como error de predicci√≥n**\n",
    "\n",
    "La recompensa intr√≠nseca surge del error del forward model:\n",
    "\n",
    "$$\n",
    "r_t^{int} = \\frac{1}{2}\n",
    "\\left\\| \n",
    "\\hat{\\phi}(s_{t+1}) - \\phi(s_{t+1}) \n",
    "\\right\\|^2\n",
    "$$\n",
    "\n",
    "Esta cantidad es alta cuando el agente encuentra **situaciones desconocidas o dif√≠ciles de predecir**, incentivando la exploraci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Recompensa total usada por la Pol√≠tica**\n",
    "\n",
    "La pol√≠tica sigue optimizando una recompensa combinada:\n",
    "\n",
    "$$\n",
    "r_t^{total} = r_t^{extrinsic} + \\lambda \\, r_t^{int}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $r_t^{extrinsic}$ es la recompensa real del entorno,\n",
    "- $r_t^{int}$ proviene del ICM,\n",
    "- $\\lambda$ controla cu√°nto \"pesa\" la curiosidad.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Interpretaci√≥n completa**\n",
    "\n",
    "Cuando el agente toma una acci√≥n $a_t$ en $s_t$ y pasa a $s_{t+1}$:\n",
    "\n",
    "1. Ambos estados se codifican:  \n",
    "   $$\\phi(s_t),\\; \\phi(s_{t+1})$$\n",
    "\n",
    "2. El **Inverse Model** aprende la acci√≥n que caus√≥ ese cambio ‚Üí hace que $\\phi(\\cdot)$ ignore *ruido o elementos incontrolables*.\n",
    "\n",
    "3. El **Forward Model** predice $\\hat{\\phi}(s_{t+1})$ a partir de $\\phi(s_t)$ y $a_t$.  \n",
    "   Su error define la curiosidad.\n",
    "\n",
    "4. La pol√≠tica maximiza la suma de recompensas extr√≠nsecas + intr√≠nsecas.\n",
    "\n",
    "**Resultado:**  \n",
    "El agente explora de forma robusta, sin distraerse con cambios aleatorios del entorno que **no est√°n afectados por sus acciones** (un beneficio clave del codificador $\\phi$).\n",
    "\n",
    "- Le da al agente **motivaci√≥n propia** para explorar, incluso cuando no hay recompensas externas.\n",
    "- Act√∫a como **preentrenamiento de exploraci√≥n**: aprende la estructura del entorno.\n",
    "- Funciona especialmente bien en **entornos con recompensas muy escasas o retrasadas**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Problemas y Limitaciones**\n",
    "\n",
    "##### **1. Exploitation de ruido**  \n",
    "Si hay una zona del entorno donde las observaciones cambian aleatoriamente, el Forward Model fallar√° siempre ‚Üí  \n",
    "intrinsic reward muy alto ‚Üí el agente entra en un *loop* buscando solo esa zona.\n",
    "\n",
    "##### **2. Aprendizajes no transferibles**  \n",
    "El agente puede aprender a explorar bien, pero no necesariamente a optimizar la tarea si:\n",
    "- el objetivo real est√° lejos,  \n",
    "- el entorno tiene din√°micas enga√±osas.\n",
    "\n",
    "##### **3. Coste adicional**  \n",
    "Entrenar IM + FM + Policy agrega complejidad computacional.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Resumen**\n",
    "\n",
    "> **Curiosity Driven Exploration** permite que el agente encuentre comportamientos √∫tiles incluso sin recompensas externas.  \n",
    "El ICM aprende qu√© partes del entorno son *dif√≠ciles de predecir* y usa ese error como una recompensa interna para impulsar la exploraci√≥n, aunque puede ser enga√±ado por entornos ruidosos.\n",
    "\n",
    "---\n",
    "### Meta-Reinforcement Learning (Meta-RL / Learning to Learn)\n",
    "\n",
    "Meta-RL busca que un agente **aprenda a adaptarse r√°pido** cuando la tarea o las reglas del entorno cambian (p. ej. bandits con probabilidades cambiantes, metas que se mueven en un laberinto, variantes de un videojuego). En vez de aprender una sola pol√≠tica para un problema fijo, se aprende una **estrategia meta** que permite obtener buenas pol√≠ticas con muy pocos pasos de interacci√≥n en una nueva tarea.\n",
    "\n",
    "> **¬øQu√© problema resuelve?**\n",
    "En entornos no estacionarios o en familias de tareas (distribuci√≥n de tareas $p(\\mathcal{T})$), los m√©todos RL tradicionales (model-free) requieren muchas interacciones para reaprender. Meta-RL intenta **aprender la estructura** entre tareas para que la adaptaci√≥n sea *r√°pida* (few-shot).\n",
    "\n",
    "Se define una distribuci√≥n de tareas $p(\\mathcal{T})$. Para cada tarea $\\mathcal{T}$ tenemos una p√©rdida/funci√≥n de rendimiento $L_{\\mathcal{T}}(\\cdot)$. El objetivo meta es:\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\; \\mathbb{E}_{\\mathcal{T}\\sim p(\\mathcal{T})}\\big[\\, L_{\\mathcal{T}}\\big(U(\\theta, \\mathcal{D}_{\\mathcal{T}})\\big)\\,\\big]\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $\\theta$ son los par√°metros meta (inicializaci√≥n, arquitect., etc.).  \n",
    "- $\\mathcal{D}_{\\mathcal{T}}$ son las pocas experiencias recolectadas en la tarea $\\mathcal{T}$.  \n",
    "- $U(\\theta,\\mathcal{D}_{\\mathcal{T}})$ es la **regla de adaptaci√≥n** (por ejemplo una actualizaci√≥n de gradiente, o la evoluci√≥n del estado oculto en una RNN).\n",
    "\n",
    "---\n",
    "\n",
    "#### Estrategias comunes en Meta-RL\n",
    "\n",
    "##### 1. **Optimization-based (p. ej. MAML for RL)**\n",
    "- Aprender una **inicializaci√≥n** de par√°metros tal que unas pocas actualizaciones de gradiente en una nueva tarea producen una buena pol√≠tica.\n",
    "\n",
    "##### 2. **Recurrent / Contextual policies (p. ej. RL¬≤, Prefrontal Network)**\n",
    "- La pol√≠tica incluye memoria (LSTM/GRU/Transformer). En lugar de actualizar par√°metros con gradiente, la **memoria interna** (estado oculto) se actualiza autom√°ticamente con secuencia de $(s,a,r)$ y codifica el *contexto* / estructura de la tarea.\n",
    "- Entrenamiento meta: expones la RNN a m√∫ltiples episodios por tarea; la RNN aprende a ‚Äúleer‚Äù se√±ales (acciones, recompensas anteriores) y adaptar comportamiento *on-the-fly*.\n",
    "\n",
    "##### 3. **Model-based meta-RL**\n",
    "- Aprender un **modelo de din√°mica** que sea compartible entre tareas y usarlo para planificar o para adaptar pol√≠tica r√°pidamente.\n",
    "- Ejemplos: adaptar par√°metros del modelo con pocas muestras y planear en ese modelo adaptado.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Arquitecturas Prefrontal Network**\n",
    "- **Entrada:** adem√°s del estado actual $s_t$, se alimentan se√±ales de contexto: acci√≥n previa $a_{t-1}$, recompensa previa $r_{t-1}$, bandera de fin de episodio, otros indicadores.  \n",
    "- **Red recurrente (LSTM/GRU):** mantiene un estado oculto $h_t$ que resume historia breve y permite inferir la tarea actual.  \n",
    "- **Salida:** pol√≠tica $\\pi(a|s,h)$ y/o critic $V(s,h)$.  \n",
    "- El LSTM act√∫a como una *memoria de meta-aprendizaje* (simula la funci√≥n de la corteza prefrontal).\n",
    "\n",
    "---\n",
    "\n",
    "#### Ejemplo intuitivo: 2-armed bandit no estacionario\n",
    "- Tarea: cada episodio, las probabilidades de los dos brazos pueden cambiar.\n",
    "- Un agente meta-entrenado aprende a usar la secuencia de recompensas recientes para inferir cu√°l brazo es mejor (sin fine-tune), gracias a su memoria (LSTM) o a una inicializaci√≥n que se adapta r√°pido (MAML).\n",
    "\n",
    "---\n",
    "\n",
    "#### ¬øPor qu√© funciona? ‚Äî Intuici√≥n\n",
    "- Muchas tareas comparten estructura (p. ej. ‚Äúhay dos tipos de din√°mica‚Äù, ‚Äúlas recompensas cambian lentamente‚Äù).  \n",
    "- Meta-RL explota esas regularidades: aprende **c√≥mo aprender** ‚Äî reglas de actualizaci√≥n o pol√≠ticas recurrentes que codifican estrategias de exploraci√≥n y explotaci√≥n eficientes bajo incertidumbre.\n",
    "\n",
    "---\n",
    "\n",
    "### **Decision Transformer (DT)**  \n",
    "Un enfoque moderno que reformula el *Reinforcement Learning* como un **problema de modelado de secuencias**, de manera similar a GPT o modelos tipo BERT.\n",
    "\n",
    "En lugar de aprender valores, TD-errors o pol√≠ticas expl√≠citas, el Decision Transformer aprende a **predecir la siguiente acci√≥n** a partir de una secuencia pasada de:\n",
    "- estados\n",
    "- acciones\n",
    "- recompensas\n",
    "- *return-to-go* (suma futura de recompensas deseada)\n",
    "\n",
    "DT aprende patrones de **trayectorias completas** usando *causal attention*.\n",
    "\n",
    "---\n",
    "\n",
    "#### **C√≥mo funciona**\n",
    "\n",
    "##### **1. Representaci√≥n como secuencia**\n",
    "Cada paso se convierte en tokens:\n",
    "\n",
    "$$\n",
    "(R_t, s_t, a_t), (R_{t+1}, s_{t+1}, a_{t+1}), \\dots\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $R_t$ = *return-to-go* (recompensa futura que queremos alcanzar)  \n",
    "- $s_t$ = estado  \n",
    "- $a_t$ = acci√≥n  \n",
    "\n",
    "El modelo recibe varios de estos tokens concatenados como **una sola secuencia**, igual que una oraci√≥n en NLP.\n",
    "\n",
    "---\n",
    "\n",
    "##### **2. Modelo Transformer**\n",
    "Como en GPT, utiliza *masked causal attention*:\n",
    "\n",
    "- solo ve el pasado  \n",
    "- predice el siguiente token: **la acci√≥n √≥ptima**  \n",
    "\n",
    "El mecanismo de atenci√≥n permite:\n",
    "- reconocer patrones largos en secuencias  \n",
    "- ignorar partes irrelevantes del estado  \n",
    "- aprender trayectorias buenas incluso cuando la mayor√≠a son sub√≥ptimas\n",
    "\n",
    "---\n",
    "\n",
    "##### **3. Predicci√≥n de la acci√≥n**\n",
    "El transformador se entrena para resolver:\n",
    "\n",
    "$$\n",
    "(a_{t}) = f_{\\text{Transformer}}(R_{t}, s_{t}, a_{t-1}, s_{t-1}, \\dots)\n",
    "$$\n",
    "\n",
    "Es decir, aprende qu√© acci√≥n llevar√≠a al *return-to-go* deseado.\n",
    "\n",
    "---\n",
    "\n",
    "##### **4. Ventajas clave**\n",
    "- No usa TD-learning ni una funci√≥n valor.  \n",
    "- Maneja datos **offline**: aprende solo de secuencias grabadas.  \n",
    "- Aprovecha *attention* para reconstruir pol√≠ticas de alto rendimiento aunque los datos vengan de comportamientos no √≥ptimos.  \n",
    "- Escala muy bien con datos masivos, igual que los modelos de lenguaje.\n",
    "\n",
    "---\n",
    "\n",
    "##### **5. Intuici√≥n**\n",
    "DT aprende:  \n",
    "> *‚ÄúEn secuencias donde el objetivo era alto, la gente que hizo esto tom√≥ estas acciones‚Ä¶ as√≠ que yo tambi√©n las tomar√©.‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "## 10. RL from Human Feedback (RLHF)\n",
    "\n",
    "Esquema para alinear IAs (como ChatGPT) con intenci√≥n humana:\n",
    "1.  **Pretraining:** Supervised Learning (predicci√≥n de next-token).\n",
    "2.  **SFT (Supervised Fine-Tuning):** Se ajusta con ejemplos de buenas preguntas/respuestas humanas.\n",
    "3.  **Reward Modeling:** Se entrena una red neuronal para predecir una puntuaci√≥n (score) basada en rankings hechos por humanos (esto es mejor que aquello).\n",
    "4.  **RL Optimization (PPO):** Se usa PPO para optimizar el modelo de lenguaje usando el Reward Model como fuente de recompensa.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Information Theory \n",
    "\n",
    "## 1. ¬øQu√© es la Informaci√≥n?\n",
    "La **informaci√≥n** se entiende como *variaciones en los datos*.  \n",
    "Un **mensaje** es un conjunto de variaciones estructuradas siguiendo un patr√≥n.\n",
    "\n",
    "El objetivo central de la teor√≠a de la informaci√≥n es:\n",
    "> **Encontrar el mejor patr√≥n para transmitir mensajes por un canal ruidoso minimizando incertidumbre y p√©rdida.**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Transmitiendo un Mensaje en un Canal Ruidoso\n",
    "Cuando enviamos un mensaje (por ejemplo, una palabra):\n",
    "\n",
    "- El canal puede **corromper o perder** partes del mensaje.  \n",
    "- El receptor debe pedir aclaraciones (‚Äú¬øqu√© letra era?‚Äù).\n",
    "- Para minimizar preguntas, debemos dise√±ar un sistema eficiente para identificar el s√≠mbolo enviado.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Reduciendo al M√≠nimo las Preguntas (Bits)\n",
    "La estrategia √≥ptima es codificar mensajes usando **dos s√≠mbolos**:  \n",
    "`0` y `1` ‚Üí m√≠nima cantidad de estados.\n",
    "\n",
    "Cada pregunta del receptor divide el espacio de posibilidades en dos:\n",
    "- ‚Äú¬øEst√° en la primera mitad?‚Äù\n",
    "- ‚Äú¬øEst√° en la segunda mitad?‚Äù\n",
    "\n",
    "Ejemplo para letras A‚ÄìZ:\n",
    "- 26 s√≠mbolos ‚Üí ¬øcu√°ntas preguntas m√≠nimas?  \n",
    "- Resolver:  \n",
    "  $$\n",
    "  2^x = 26 \\quad \\Rightarrow \\quad x = \\log_2 26 ‚âà 4.7 \\text{ bits}\n",
    "  $$\n",
    "\n",
    "Ejemplo para un mazo de 52 cartas:\n",
    "$$\n",
    "x = \\log_2 52 ‚âà 5.7 \\text{ bits}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Informaci√≥n Total de un Mensaje\n",
    "\n",
    "La **informaci√≥n total** de un mensaje depende de:\n",
    "\n",
    "- $n$: n√∫mero de s√≠mbolos a transmitir  \n",
    "- $s$: n√∫mero de s√≠mbolos posibles (tama√±o del alfabeto o conjunto de opciones)\n",
    "\n",
    "La f√≥rmula para calcular la informaci√≥n total en **bits** es:\n",
    "\n",
    "$$\n",
    "I = n \\cdot \\log_2(s)\n",
    "$$\n",
    "\n",
    "### Ejemplos:\n",
    "\n",
    "1. **Transmisi√≥n de letras:**\n",
    "   - Alfabeto de 26 letras\n",
    "   - Mensaje de 6 letras\n",
    "   - Cada letra requiere en promedio:\n",
    "     \n",
    "     $$\n",
    "     \\log_2(26) \\approx 4.7 \\text{ bits}\n",
    "     $$\n",
    "     \n",
    "   - Informaci√≥n total del mensaje:\n",
    "     \n",
    "     $$\n",
    "     I = 6 \\cdot 4.7 \\approx 28.2 \\text{ bits}\n",
    "     $$\n",
    "\n",
    "2. **Transmisi√≥n de cartas de un mazo:**\n",
    "   - Mazo de 52 cartas\n",
    "   - Mensaje de 5 cartas\n",
    "   - Cada carta requiere en promedio:\n",
    "     \n",
    "     $$\n",
    "     \\log_2(52) \\approx 5.7 \\text{ bits}\n",
    "     $$\n",
    "     \n",
    "   - Informaci√≥n total del mensaje:\n",
    "     \n",
    "     $$\n",
    "     I = 5 \\cdot 5.7 \\approx 28.5 \\text{ bits}\n",
    "     $$\n",
    "\n",
    "> **Interpretaci√≥n:** Cuantos m√°s s√≠mbolos posibles tenga el conjunto ($s$), m√°s bits se necesitan para transmitir cada s√≠mbolo. Esta medida refleja la **incertidumbre** de cada mensaje antes de ser transmitido.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Informaci√≥n con Probabilidades Desiguales\n",
    "\n",
    "Cuando los s√≠mbolos no son igualmente probables, debemos calcular la **informaci√≥n promedio** usando **valores esperados**.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo: 4 s√≠mbolos con probabilidades distintas\n",
    "\n",
    "Supongamos:\n",
    "\n",
    "$$\n",
    "P(A) = 0.5, \\quad P(B) = 0.125, \\quad P(C) = 0.125, \\quad P(D) = 0.25\n",
    "$$\n",
    "\n",
    "Para identificar un s√≠mbolo, el n√∫mero de preguntas se ajusta seg√∫n la probabilidad:\n",
    "\n",
    "1. Primero se pregunta si es $A$:  \n",
    "   - Probabilidad = 0.5 ‚Üí **1 pregunta**\n",
    "2. Luego $D$:  \n",
    "   - Probabilidad = 0.25 ‚Üí **2 preguntas**\n",
    "3. Finalmente $B$ y $C$:  \n",
    "   - Probabilidad = 0.125 cada uno ‚Üí **3 preguntas**  \n",
    "\n",
    "---\n",
    "\n",
    "### C√°lculo del n√∫mero esperado de preguntas\n",
    "\n",
    "$$\n",
    "\\#\\text{questions} = P(A) \\cdot 1 + P(D) \\cdot 2 + P(B) \\cdot 3 + P(C) \\cdot 3\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\#\\text{questions} = 0.5 \\cdot 1 + 0.25 \\cdot 2 + 0.125 \\cdot 3 + 0.125 \\cdot 3 = 1.75\n",
    "$$\n",
    "\n",
    "> Esto representa el **n√∫mero promedio de preguntas** necesarias para identificar un s√≠mbolo en este conjunto no uniforme.\n",
    "\n",
    "---\n",
    "\n",
    "### Entrop√≠a\n",
    "\n",
    "La **entrop√≠a** $H$ mide la incertidumbre promedio de la distribuci√≥n:\n",
    "\n",
    "$$\n",
    "H = - \\sum_i p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Aplicando al ejemplo:\n",
    "\n",
    "$$\n",
    "H = - [0.5 \\log_2 0.5 + 0.25 \\log_2 0.25 + 0.125 \\log_2 0.125 + 0.125 \\log_2 0.125] \\approx 1.75 \\text{ bits}\n",
    "$$\n",
    "\n",
    "**Interpretaci√≥n:**\n",
    "\n",
    "- Mayor probabilidad de un s√≠mbolo ‚Üí menos preguntas necesarias.  \n",
    "- Menor entrop√≠a $H$ ‚Üí menor incertidumbre del mensaje.  \n",
    "- La entrop√≠a refleja **la cantidad promedio de informaci√≥n** necesaria para transmitir un mensaje considerando probabilidades desiguales.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Entrop√≠a\n",
    "La **entrop√≠a** mide cu√°nta incertidumbre existe en una distribuci√≥n:\n",
    "\n",
    "$$\n",
    "H = - \\sum_{i} p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "Interpretaci√≥n:\n",
    "- **H alta** ‚Üí alta incertidumbre, s√≠mbolos equiprobables.  \n",
    "- **H baja** ‚Üí hay s√≠mbolos mucho m√°s probables que otros.\n",
    "\n",
    "Ejemplos:\n",
    "- Distribuci√≥n de sexo 30‚Äì34 a√±os: $H = 0.99$\n",
    "- Distribuci√≥n en militares (91% hombres): $H = 0.43$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Information Gain (Ganancia de Informaci√≥n)\n",
    "Es la reducci√≥n de entrop√≠a al conocer un atributo:\n",
    "\n",
    "$$\n",
    "IG = H(D) - H(D|a)\n",
    "$$\n",
    "\n",
    "Ejemplo:\n",
    "$$\n",
    "0.99 - 0.94 = 0.05\n",
    "$$\n",
    "\n",
    "Muy utilizado en:\n",
    "- √Årboles de decisi√≥n\n",
    "- Selecci√≥n de atributos\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Divergencia KL (Kullback‚ÄìLeibler)\n",
    "Mide cu√°n diferente es una distribuci√≥n **P** de una distribuci√≥n **Q**:\n",
    "\n",
    "$$\n",
    "KL(P‚ÄñQ) = \\sum_i P(x_i) \\log \\frac{P(x_i)}{Q(x_i)}\n",
    "$$\n",
    "\n",
    "Propiedad clave:\n",
    "- **No es sim√©trica:**  \n",
    "  $$\n",
    "  KL(P‚ÄñQ) \\neq KL(Q‚ÄñP)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Cross-Entropy (P√©rdida en Redes Neuronales)\n",
    "Usada en clasificaci√≥n y modelos probabil√≠sticos, otro m√©todo para calcular la diferencia entre dos distribuciones:\n",
    "\n",
    "$$\n",
    "H(P,Q) = - \\sum_i P(x_i)\\log Q(x_i)\n",
    "$$\n",
    "\n",
    "Interpretaci√≥n:\n",
    "- P = la *verdad* (dataset)\n",
    "- Q = nuestro *modelo*\n",
    "- Cross-Entropy dice cu√°n mal Q aproxima a P.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Relaci√≥n entre Verdad, Datos y Modelo\n",
    "En Machine Learning:\n",
    "\n",
    "$$\n",
    "P(modelo) \\approx P(datos) \\approx P(verdad)\n",
    "$$\n",
    "\n",
    "- La **verdad** es incognoscible.  \n",
    "- Los **datos** son nuestro proxy.  \n",
    "- El **modelo** intenta aproximarlos.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
