
---

# **Gu√≠a - Reinforcement Learning**
- Elaborado por: Miguel P√©rez

---
# Tipos de Datos

### **Datos Discretos (enteros)**
- Valores contables  
- Ej: edad, n√∫mero de fallas

### **Datos Categ√≥ricos**
- **Binomiales:** 2 categor√≠as (s√≠/no, hombre/mujer)  
- **Multinomiales:** >2 categor√≠as (tipo de combustible, color de ojos)

### **Datos Continuos**
- Valores en un intervalo infinito  
- Ej: temperatura, humedad, velocidad, voltaje

---

## Histogramas
- Representan **frecuencias** de valores o rangos.  
- Sirve para:
  - Entender distribuci√≥n  
  - Ver patrones  
  - Detectar valores comunes o raros  

---

# Distribuciones

### **Distribuci√≥n Uniforme**
- Todos los valores tienen la MISMA probabilidad.  
- Ej: dado justo, baraja mezclada.

### **Distribuci√≥n Binomial**
- Solo dos resultados posibles: √©xito / fracaso.  
- Ej: lanzar una moneda n veces.

### **Distribuci√≥n Multinomial**
- M√°s de dos categor√≠as excluyentes.  
- Ej: salidas de las caras de un dado.

### **Distribuci√≥n Normal (Gaussiana)**
- Forma de campana.  
- Media = mediana = moda  
- Com√∫n en fen√≥menos naturales.

### **Distribuci√≥n Poisson**
- Cuenta eventos en un intervalo.  
- Ej: llamadas por minuto, gotas por segundo.

### **Distribuci√≥n Pareto**
- Describe fen√≥menos 80-20.  
- Ej: riqueza, producci√≥n, ventas.

### **Distribuci√≥n Beta**
- Variable continua entre 0 y 1.  
- Modela proporciones.

---

# Medidas Estad√≠sticas

### **Moda**
Valor que m√°s se repite.

### **Media**
$$
\text{mean} = \frac{\sum x}{n}
$$

### **Mediana**
Valor central del conjunto ordenado.

### **Varianza**
Como estan dispersos los datos en una distribuci√≥n.
$$
\sigma^2 = \frac{\sum (x-\mu)^2}{n}
$$

### **Desviaci√≥n Est√°ndar**
Indica los rangos en los que los datos se presentan en mayor proporci√≥n.
$$
\sigma = \sqrt{\text{Var}}
$$


---

# Teorema Central del L√≠mite (CLT)

- Si tomas **muchas muestras** de cualquier distribuci√≥n,
  la **distribuci√≥n de sus medias ser√° Normal**.
- Usado en:
  - Ciencia
  - Encuestas
  - ML: Policy Gradients, m√©todos con expected value

---

# Probabilidad

$$
p(x) = \frac{\text{veces que ocurre}}{\text{total de eventos}}
$$

---

# PDF ‚Äî Probability Density Function

Each distribution type has a function called the Probability Density Function (PDF) which intends to model the density of a given dataset and return a number between 0 and 1 that signals how dense the data is. Each distribution has its own PDF equation.

## **PDF Gaussiana / Discretos**
$$
\mu = mean
$$
$$
\sigma = Std Dev
$$
$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

## **PDF Binomial / Categ√≥rica**

$$
\beta = Spread (modifies\;sigmoid-like \;curve)
$$
$$a_1, a_2, a_N = parameters$$

$$
P(a_1, a_2, \beta) =  \sigma(\beta(a_1-a_2)) = \frac{1}{1+e^{-x}}
$$

$$
P(a_1, a_2,...,a_N) =  \frac{e^{ a_i }}{\sum e^{ a_j }}
$$



---

# CDF ‚Äî Cumulative Distribution Function

Da la probabilidad de que:  
$$
X \leq x
$$

## Outliers (IQR)
$$
\text{IQR} = Q3 - Q1
$$

Outlier si:
$$
x > Q3 + 1.5(IQR)
$$
o  
$$
x < Q1 - 1.5(IQR)
$$

---

# Valor Esperado (Expected Value)

## Sin pesos:
$$
E[X] = \frac{1}{N}\sum x_i
$$

## Con pesos/p(x):
Average of data transformed by a function (e.g.
log(x)) weighed by its likelihood (i.e. p(x))
$$
E[f(X)] = \sum p(x_i)\, f(x_i)
$$

---

# Multi-Armed Bandit (Tragamonedas)

Actualizaci√≥n incremental del valor esperado:

$$
Q_k = Q_{k-1} + \frac{1}{k}(r_k - Q_{k-1})
$$

---

# Exploration vs Exploitation

## **Epsilon-Greedy**
$$
\epsilon_t = \epsilon_{end} + (\epsilon_{start}-\epsilon_{end}) e^{-t/\text{decay}}
$$

- random > Œµ ‚Üí explotaci√≥n  
- random ‚â§ Œµ ‚Üí exploraci√≥n  

## **Softmax (Boltzmann) o Sigmoid para muestreo**
$$
P(a) = \frac{e^{\beta Q(a)}}{\sum e^{\beta Q(a')}}
$$
$$
P(Q, \beta) =  \sigma(\beta(Q_a-Q_b)) = \frac{1}{1+e^{-\beta (Q_a-Q_b)}}
$$
Œ≤ par√°metro de control (‚Äútemperature‚Äù) controla qu√© tan ‚Äúdeterminista‚Äù es la elecci√≥n.

---

# Reglas de Probabilidad

## **Suma ( two mutually exclusive events happening):**
$$
P(A \cup B) = P(A) + P(B)
$$

## **Producto ( both events happening ):**
$$
P(A \cap B) = P(A)P(B)
$$

## **Probabilidad Condicional**
Compute the probability of A
given the ocurrence of B. This means that
B must happen first, subject to its own
uncertainty, and only then, from what is
left, A can happen with a given
probability
$$
P(A|B) = \frac{P(A\cap B)}{P(B)}
$$

Base del Teorema de Bayes.

---

#  Maximum Likelihood ‚Äî Apuntes

## Probabilidad vs Likelihood
- **Probabilidad**: dado un modelo (par√°metros conocidos), ¬øqu√© tan probable es que X tome ciertos valores?  
- **Likelihood (verosimilitud)**: dada una observaci√≥n $x$ y una familia de distribuciones parametrizadas por $\theta$, la verosimilitud es:

$$
L(\theta) = f(x \mid \theta)
$$

## PDF normal (Gaussiana)
$$
f(x \mid \mu, \sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

## Maximum Likelihood
Para datos $x_1,\dots,x_n$ y par√°metros $\theta$:

$$
L(\theta) = \prod_{i=1}^n f(x_i \mid \theta), \qquad
\ln L(\theta) = \sum_{i=1}^n \ln f(x_i \mid \theta)
$$

Use logarithms to simplify computations and make use of its concave property

## Distribuci√≥n Normal donde $\theta=[\mu,\sigma^2]$

Log-likelihood:
$$
\ln L(\mu,\sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2
$$

Derivando parcialmente e igualando a cero:

- Estimador batch de la media (MLE):
  $$
  \hat{\mu} = \frac{1}{n}\sum_{i=1}^n x_i
  $$

- Estimador batch de la varianza (MLE):
  $$
  \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \hat{\mu})^2
  $$

## Optimizaci√≥n por gradiente
- Calcula $\nabla_\theta \ln L(\theta)$ y usa descenso por gradiente:

  $$
  \theta \leftarrow \theta + \eta \nabla_\theta \ln L(\theta)
  $$

  (o la variante con signo negativo si minimizas la *neg-log-likelihood*).

## Estimaci√≥n secuencial (online)
- Media incremental:
  $$
  \hat{\mu}_{t+1} = \hat{\mu}_t + \frac{1}{N}(x_{t+1} - \hat{\mu}_t)
    $$
- Varianza incremental (forma simple mostrada):
  $$
  \hat{\sigma}^2_{t+1} = \hat{\sigma}^2_t + \frac{(x_{t+1} - \hat{\mu}_t)^2 - \hat{\sigma}^2_t}{t+1}
  $$
  $$
  \hat{\sigma}^2_{t+1} = \hat{\sigma}^2_t + \frac{1}{N}((x_t - \hat{\mu}) - \hat{\sigma}^2_t)
  $$
  
---

# Multinomial Distribution Approximation
## Multinomial PDF

Sea un conjunto de par√°metros $a_k$, donde $a_j$ es el par√°metro asociado a la categor√≠a cuya probabilidad queremos modelar.

La probabilidad de la categor√≠a $j$ est√° dada por:

$$
S_j = \frac{e^{\beta a_j}}{\sum_{i=1}^{n} e^{\beta a_i}}
$$

donde $\beta$ es un par√°metro de control (similar al "inverse temperature").

---

## Optimization (Maximum Likelihood)

Queremos ajustar los par√°metros $\{a_1, a_2, ..., a_k, \beta\}$ para aproximar correctamente la distribuci√≥n multinomial.

La PDF general:

$$
\text{pdf}(x \mid \theta) = \frac{e^{\beta a_x}}{\sum_{i=1}^{n} e^{\beta a_i}}
$$

La likelihood del dataset:

$$
L(x_1,\dots,x_n \mid \theta) = \prod_{i=1}^{n} \text{pdf}(x_i\mid\theta)
$$

### Derivadas parciales (softmax)

Derivada del score $S_j$ respecto a su par√°metro:

- **Cuando $i = j$**:

$$
\frac{\partial S_j}{\partial a_j} = S_j(1 - S_j)
$$

- **Cuando $i \neq j$**:

$$
\frac{\partial S_j}{\partial a_i} = -S_j S_i
$$

Estas son exactamente las derivadas del **softmax** est√°ndar.

---

# Optimization via Policy Learning

En un agente RL, si cada acci√≥n corresponde a una categor√≠a de la distribuci√≥n multinomial, la pol√≠tica viene dada por:

$$
p(a_i) = \frac{e^{\beta a_i}}{\sum_{k=1}^n e^{\beta a_k}}
$$

El **valor esperado del retorno** es:

$$
\bar{r} = \sum_{i=1}^n p(a_i)\, r_i
$$

---

## Gradient of Expected Reward

El gradiente del valor esperado respecto al par√°metro $a_i$ es:

$$
\frac{\partial \bar{r}}{\partial a_i}
= \beta\, p(a_i)\,\left(r_i - \bar{r}\right)
$$

Esta es la forma cl√°sica:

> **Policy Gradient = Softmax Gradient √ó Advantage**

donde el *advantage* es $r_i - \bar{r}$.

---

## Parameter update rules

### Regla de actualizaci√≥n general:

$$
a_i \leftarrow a_i + \lambda\, \frac{\partial \bar{r}}{\partial a_i}
$$

Sustituyendo el gradiente:

$$
a_i \leftarrow a_i + \lambda \beta\, p(a_i)\,(r_i - \bar{r})
$$

---



## Alternativa por casos (como en la presentaci√≥n)

Cuando la acci√≥n tomada es $i$:

$$
a_i \leftarrow a_i + \lambda (1 - p(a_i))(r_i - \bar{r})
$$

Para todas las acciones no tomadas ($j \neq i$):

$$
a_j \leftarrow a_j - \lambda\, p(a_j)(r_i - \bar{r})
$$

Estas f√≥rmulas equivalen a la derivada del softmax *policy gradient*.

---

# Reinforcement Learning (RL)

---

## Introducci√≥n y Fundamentos

### Machine Learning: Comparativa
* **Supervised Learning:** Tenemos datos etiquetados por humanos (Input $\to$ Target).
* **Unsupervised Learning:** Tenemos datos, pero no etiquetas (buscamos patrones/estructuras).
* **Reinforcement Learning:** **No tenemos datos previos**. Tenemos un **agente** y un **entorno** que provee **recompensas**.

### Elementos del RL
El ciclo b√°sico de interacci√≥n:
1.  **Agente:** Entidad artificial que analiza observaciones y emite acciones.
2.  **Entorno (Environment):** Sistema que recibe la acci√≥n, transiciona a un nuevo estado y emite una observaci√≥n y una recompensa.
3.  **Recompensa (Reward):** Se√±al escalar que indica qu√© tan buena fue la acci√≥n con respecto a un objetivo.
4.  **Pol√≠tica (Policy):** La estrategia del agente (mapeo de observaciones a acciones).

> **El Problema Central:** C√≥mo observar, recolectar y analizar datos para emitir acciones que **maximicen la recompensa acumulada**.

### Tipos de Motivaci√≥n
* **Extr√≠nseca:** La recompensa es dise√±ada por humanos (ingenier√≠a de recompensas) para guiar al agente (ej. puntos en un juego).
* **Intr√≠nseca:** Se√±al generada por el propio agente para fomentar la exploraci√≥n.
    * *Curiosidad:* Basada en el error de predicci√≥n (si no puedo predecir qu√© pasar√°, quiero ir ah√≠).
    * *Empowerment:* Capacidad de controlar el entorno.

---

## Tipos de Enfoques en RL

### Por Modelo
1.  **Model-Free (Libre de modelo):** Mapea observaciones directamente a acciones o valores usando prueba y error. No intenta entender "c√≥mo funciona la f√≠sica" del entorno.
2.  **Model-Based (Basado en modelo):**
    * Entrena un modelo para predecir la din√°mica del entorno (Estado actual + Acci√≥n $\to$ Siguiente Estado).
    * Usa ese modelo para planificar o entrenar una pol√≠tica.

### Por M√©todo de Aprendizaje
1.  **Value-based:** Aprende el valor num√©rico de estar en un estado o tomar una acci√≥n ($Q$). Elige la acci√≥n con mayor valor.
2.  **Policy-based:** Aprende directamente la funci√≥n de probabilidad de las acciones dado un estado.
3.  **Actor-Critic:** H√≠brido. Un *Actor* decide la acci√≥n y un *Cr√≠tico* estima el valor de esa acci√≥n para ajustar al actor.

---

## Entornos y Gymnasium

### Tipos de Entornos
* **K-armed Bandits:** Tragamonedas. Elegir opciones con diferentes probabilidades de recompensa (sin estados secuenciales).
* **Mazes (Laberintos):** Espacio navegable con obst√°culos y metas.
* **Robots:** Sistemas mec√°nicos (caminar, agarrar). Control motor continuo.
* **Juegos:** StarCraft, Atari. Usados para testear algoritmos (benchmarks).

### Estructura Gymnasium (Python Wrapper)
Librer√≠a est√°ndar para entornos de RL. Clase principal `CustomEnv`:
* `__init__()`: Constructor, define variables iniciales.
* `reset()`: Restaura el entorno al inicio y devuelve la primera observaci√≥n.
* `step(action)`: Ejecuta una acci√≥n. Retorna:
    * `observation`: Nuevo estado.
    * `reward`: Recompensa obtenida.
    * `terminated/truncated`: Booleano (¬øtermin√≥ el juego?).

---

## Value-Based Methods (M√©todos Basados en Valor)

### Concepto Biol√≥gico
Inspirado en la dopamina. Las neuronas refuerzan sinapsis cuando la recompensa recibida es mayor a la esperada (error de predicci√≥n positivo).

### Expected Value (Valor Esperado)
Es el promedio ponderado de los resultados posibles.
$$E[f] = \sum p(x_i) f(x_i)$$
* Donde $p(x_i)$ es la probabilidad de que ocurra el evento y $f(x_i)$ el valor del evento.

### Q-Learning (Tabular)
El cerebro/agente modela el valor esperado de las opciones.

**Actualizaci√≥n de Valor (Simple):**
$$Q_k = Q_{k-1} + \alpha (r_k - Q_{k-1})$$
* $Q_k$: Valor acumulado.
* $\alpha$ (o $1/k$): Tasa de aprendizaje (Learning Rate).
* $r_k$: Recompensa actual.
* **Interpretaci√≥n:** El nuevo valor es el viejo valor m√°s una fracci√≥n del "error" (diferencia entre lo que recib√≠ y lo que cre√≠a que iba a recibir).

**Valor Relativo (Bavard et al.):**
El cerebro normaliza los valores bas√°ndose en el contexto (min y max recompensas disponibles).
$$Q_k = Q_{k-1} + \alpha \left( \frac{r_{obj} - r_{min}}{r_{max} - r_{min}} - Q_{k-1} \right)$$

### Temporal Difference (TD) y Ecuaci√≥n de Bellman
Para decisiones secuenciales (donde el futuro importa).
**Ecuaci√≥n Clave:**
$$Q(s, a)_{new} = Q(s, a)_{old} + \alpha \underbrace{[r + \gamma \cdot \max Q(s', a') - Q(s, a)_{old}]}_{\text{TD Error}}$$

* $\gamma$ (Gamma): Factor de descuento. Qu√© tanto me importa el futuro vs el presente.
* $\max Q(s', a')$: La mejor suposici√≥n del valor del *siguiente* estado.

---

## Markov Decision Process

Un MDP es un **caso especial de las Cadenas de Markov**.
* **Cadena de Markov normal:** Las transiciones ocurren de forma estoc√°stica "porque s√≠" (fen√≥menos naturales).
* **MDP:** Las transiciones son provocadas por una **fuente externa** (Agente o Usuario). El sistema no cambia de estado a menos que se ejecute una acci√≥n ($a$).

### Ciclo de Interacci√≥n
1.  **Estado Actual:** El sistema est√° en un estado $S$.
2.  **Acci√≥n Externa:** El agente selecciona una acci√≥n ($a_{ij}$) de una matriz de acciones posibles.
3.  **Transici√≥n y Recompensa:** El sistema cambia al siguiente estado y "devuelve" una recompensa ($r_{ij}$).

### La Propiedad de Markov (The Markovian Assumption)
Es la regla de oro para que las matem√°ticas funcionen. Establece que el futuro es independiente del pasado, dado el presente.
> *"La probabilidad de pasar al siguiente estado y obtener una recompensa depende **√∫nicamente** del estado actual y la acci√≥n tomada, no de la historia previa."*

$$P(S_{t+1} | S_t, a_t, S_{t-1}, ...) = P(S_{t+1} | S_t, a_t)$$

---

## Exploration vs Exploitation

El dilema: ¬øPruebo algo nuevo (aprender) o elijo lo que s√© que funciona (ganar recompensa)?

1.  **Epsilon-Greedy ($\epsilon$-greedy):**
    * Tirar un dado. Si sale bajo ($\epsilon$), elijo una acci√≥n aleatoria (Exploraci√≥n).
    * Si sale alto, elijo la mejor acci√≥n conocida (Explotaci√≥n).
    * *Decay:* $\epsilon$ empieza alto (mucho random) y baja con el tiempo.

2.  **Softmax / Sigmoide:**
    * Convierte los valores $Q$ en probabilidades. Si una acci√≥n es mucho mejor, tiene mucha m√°s probabilidad de ser elegida, pero no el 100%.
    * $$P(a) = \frac{e^{Q(a)/\tau}}{\sum e^{Q(b)/\tau}}$$

---

## Policy Gradients (Gradientes de Pol√≠tica)

### Objetivo
Optimizar directamente los par√°metros ($\theta$) de la pol√≠tica $\pi$ para maximizar la recompensa total esperada ($J$).

Parametrizamos la pol√≠tica como $\pi_\theta(a|s)$ y queremos **maximizar** el retorno esperado sobre trayectorias $\tau$:

$$
J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\big[ R(\tau) \big]
$$

donde $R(\tau)=\sum_{t=0}^{T} r_t$. Aqu√≠ $p_\theta(\tau)$ es la probabilidad de la trayectoria bajo la pol√≠tica y la din√°mica del entorno.

## 2) Log-derivative trick

Queremos $\nabla_\theta J(\theta)$.

$$
\begin{aligned}
\nabla_\theta J(\theta)
&= \nabla_\theta \int p_\theta(\tau)\, R(\tau)\, d\tau \\
&= \int \nabla_\theta p_\theta(\tau)\, R(\tau)\, d\tau
\end{aligned}
$$

Usamos el truco de derivada logar√≠tmica:

$$
\nabla_\theta p_\theta(\tau)=p_\theta(\tau)\nabla_\theta \log p_\theta(\tau)
$$

Sustituyendo:

$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta}\big[ \nabla_\theta \log p_\theta(\tau)\, R(\tau) \big]
$$

---

## 3) Factorizaci√≥n por pasos de tiempo

La probabilidad de una trayectoria:

$$
p_\theta(\tau)=p(s_0)\prod_{t=0}^{T} \pi_\theta(a_t|s_t)\, p(s_{t+1}|s_t,a_t)
$$

Tomando logaritmo:

$$
\log p_\theta(\tau)=\sum_{t=0}^{T} \log \pi_\theta(a_t|s_t) + \text{const}
$$

Por tanto, la Ecuaci√≥n del Gradiente:

$$
\nabla_\theta J(\pi_\theta)=
\mathbb{E}_{\tau\sim p_\theta}
\left[
\sum_{t=0}^{T}
\nabla_\theta \log \pi_\theta(a_t|s_t)\,\cdot R(\tau)
\right]
$$

* **Interpretaci√≥n:** Ajustamos los par√°metros $\theta$ para hacer m√°s probables las acciones $(a_t)$ que resultaron en una alta recompensa acumulada $R(\tau)$.

## Problemas Comunes en Policy Gradient
### 1. **Alta Varianza en el Gradiente**

Los m√©todos de Policy Gradient estiman el gradiente esperado de la recompensa:

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s)\, R \right]
$$

El problema es que la estimaci√≥n:

$$
\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N 
\nabla_\theta \log \pi_\theta(a_i|s_i) R_i
$$

tiene **varianza muy alta**, especialmente cuando:

- $R$ depende de trayectorias largas
- el espacio de estados es grande
- las pol√≠ticas cambian demasiado entre actualizaciones

Esto provoca **inestabilidad**, actualizaciones ruidosas y aprendizaje lento.

---

### 2. **Exploraci√≥n Ineficiente**

La pol√≠tica se actualiza en la direcci√≥n de acciones que han dado buena recompensa:

$$
\nabla_\theta \log \pi_\theta(a|s) R
$$

Si la pol√≠tica inicial es mala y produce pocas acciones con recompensa:

- el gradiente es peque√±o
- la pol√≠tica no explora lo suficiente
- se queda atrapada en √≥ptimos locales


---

### 3. **Sensibilidad a Hiperpar√°metros**

Especialmente al *learning rate*:

- si es muy peque√±o ‚Üí aprendizaje extremadamente lento  
- si es muy grande ‚Üí divergencia

---

### Baseline: Resolver Problema de Varianza
Los gradientes puros son muy ruidosos.
* **Baseline:** Restar un valor base para reducir varianza de obtenci√≥n de reward y evitar sobrepremitar una acci√≥n ineficiente. 
* **Advantage Function ($A$):**
    $$A(s, a) = Q(s, a) - V(s)$$
    * ¬øQu√© tanto mejor es esta acci√≥n comparada con el promedio de estar en este estado?

  
Existen tres variantes cl√°sicas:


#### 1. Baseline Global: **Promedio de Rewards Obtenidos**

Se utiliza un valor escalar $b$ que promedia todos los rewards obtenidos en episodios recientes:

$$
b = \frac{1}{N}\sum_{i=1}^N R_i
$$

Gradiente actualizado:

$$
\nabla_\theta J = \mathbb{E}\left[ \nabla_\theta \log \pi_\theta(a|s)(R - b) \right]
$$


#### 2. Baseline por Acci√≥n: **Promedio de Reward por Acci√≥n (Q-value)**

Aqu√≠ se usa como baseline el valor esperado de tomar una acci√≥n en un estado:

$$
b(s,a) = Q(s,a)
$$

El gradiente se vuelve:

$$
\nabla_\theta J = \mathbb{E}\left[ \nabla_\theta \log \pi_\theta(a|s)(R - Q(s,a)) \right]
$$

Este baseline corresponde directamente a la idea de **estimadores del Q-value**.

#### 3. Advantage Baseline: **$Q(s,a)$ Menos Promedio Global de Rewards**

Esta variante mezcla las dos anteriores:

$$
b = \mathbb{E}[R], \qquad A(s,a) = Q(s,a) - b
$$

Entonces el gradiente utiliza:

$$
\nabla_\theta J = \mathbb{E}\left[ \nabla_\theta \log \pi_\theta(a|s)\, A(s,a) \right]
$$

#### **Tabla Comparativa**

| Variante | F√≥rmula | Ventajas | Desventajas |
|---------|---------|----------|-------------|
| **Promedio global de rewards** | $R - b$ | F√°cil, reduce varianza | Ignora acci√≥n/estado |
| **Promedio por acci√≥n (Q-value)** | $R - Q(s,a)$ | Modela calidad real de acciones | Costoso; depende del critic |
| **Q-value ‚àí promedio global** | $Q(s,a) - b$ | Combina ambas ventajas | Requiere estimar $Q$ correctamente |

---

## Actor-Critic y Algoritmos de Optimizaci√≥n Avanzados

Cuando entrenas una pol√≠tica estoc√°stica, las acciones se eligen al azar seg√∫n sus probabilidades.
A veces, por pura suerte, una acci√≥n mala puede recibir muchas recompensas positivas en una trayectoria espec√≠fica.

¬øConsecuencia?
El algoritmo de policy gradient ajusta la pol√≠tica para favorecer m√°s esa acci√≥n mala, porque observa que ‚Äúdio buen reward‚Äù, aunque en realidad no sea buena.

Esto genera que la pol√≠tica nueva cambie demasiado respecto a la anterior, inclin√°ndose hacia acciones que aparentemente funcionaron, pero que no son realmente las mejores.

### Actor-Critic

El m√©todo **Actor-Critic** combina dos ideas clave:

1. **Actor ($\pi_\theta$):** Red neuronal que decide qu√© acci√≥n tomar. (Pol√≠tica) 
2. **Critic ($V_\phi$):** Red neuronal que estima el valor del estado ($V(s)$) para calcular el *Advantage* y reducir la varianza del gradiente.

Juntos permiten entrenar pol√≠ticas estoc√°sticas m√°s estables que el m√©todo REINFORCE tradicional.

---

### ¬øPor qu√© Actor-Critic?

El problema del Policy Gradient puro (REINFORCE) es la **alta varianza del t√©rmino $R(\tau)$**, lo que causa actualizaciones ruidosas y aprendizaje lento.

Para solucionar esto:

- Se introduce un **Critic** que estima el valor esperado del estado $V(s)$.
- Este valor sirve como baseline para calcular el *Advantage*:
  
$$
A(s,a) = R(\tau) - V_\phi(s)
$$

Esto estabiliza el gradiente y acelera la convergencia.

---

### Entrenamiento del Critic (Estimador de $V_\phi$)

El Critic se entrena para aproximar la funci√≥n de valor mediante regresi√≥n:

$$
V_\phi(s) \approx \mathbb{E}[R(\tau) \mid s]
$$

La actualizaci√≥n del Critic es:

$$
\phi = \phi + \nabla_\phi \left( \| \hat{V}_\phi(s) - R(\tau) \|_2^2 \right)
$$

Es decir:

- Se minimiza el **error cuadr√°tico** entre la predicci√≥n del Critic y la recompensa real.
- Esto convierte al Critic en un baseline adaptativo que aprende con la experiencia.

---

### Entrenamiento del Actor (Actualizaci√≥n de Pol√≠tica)

El Actor ajusta los par√°metros $\theta$ siguiendo un gradiente de pol√≠tica ponderado por el *Advantage* estimado:

$$
\theta = \theta + \nabla_\theta \log \pi_\theta(a \mid s)\, \left( R(\tau) - V_\phi(s) \right)
$$

Interpretaci√≥n:

- Si una acci√≥n $a$ produjo un reward **mayor** que lo que esperaba el Critic ‚Üí la pol√≠tica debe aumentar su probabilidad.
- Si produjo un reward **peor** de lo esperado ‚Üí la pol√≠tica debe disminuir su probabilidad.

---

### ¬øQu√© est√° pasando matem√°ticamente?

**Critic:** intenta responder  
> ‚Äú¬øQu√© tan bueno es este estado en general?‚Äù

**Actor:** intenta responder  
> ‚Äú¬øDeber√≠a repetir esta acci√≥n en estados similares?‚Äù

El *Advantage* los conecta:

$$
A(s,a) = R(\tau) - V_\phi(s)
$$

Con esto:

- **El Critic reduce la varianza** del gradiente.
- **El Actor recibe un gradiente m√°s preciso y con menos ruido.**

Actor‚ÄìCritic NO controla cu√°nto cambia la pol√≠tica entre updates.

El problema descrito originalmente es:

- La pol√≠tica nueva puede ser demasiado diferente a la vieja.

- Actor-Critic no tiene un mecanismo para limitar la magnitud del cambio de pol√≠tica.

---
### Trust Region & PPO

#### Problema principal
En *Policy Gradient* tradicional, el update sobre los par√°metros $\theta$ puede ser tan grande que:
- la nueva pol√≠tica se aleja demasiado de la anterior,
- el agente ‚Äúolvida‚Äù comportamientos √∫tiles,
- el entrenamiento se vuelve inestable y puede **divergir**.

Para evitarlo nacen los **M√©todos de Optimizaci√≥n por Regiones de Confianza** (Trust Region Methods):

### **TRPO ‚Äî Trust Region Policy Optimization**

TRPO define una regi√≥n matem√°tica en el espacio de soluciones dentro de la cual los par√°metros pueden moverse sin destruir la pol√≠tica previa.

#### üîπ Idea central  
Limitar cu√°nto puede cambiar la pol√≠tica nueva respecto a la pol√≠tica vieja:
- usando una restricci√≥n de **KL Divergence**,
- para evitar saltos demasiado grandes.

La actualizaci√≥n maximiza un objetivo nuevo, **pero bajo la restricci√≥n**:

$$
D_{KL}(\pi_{\theta_{\text{old}}} \;\|\; \pi_{\theta_{\text{new}}}) \le \delta
$$

donde:
- $D_{KL}$ mide cu√°nta informaci√≥n cambia entre pol√≠ticas,
- $\delta$ es un l√≠mite m√°ximo permitido.

#### üîπ Nuevo objetivo optimizado
TRPO maximiza una versi√≥n corregida del *surrogate objective*:

$$
L^{TRPO}(\theta) = 
\mathbb{E}\left[
\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A^{\pi_{\text{old}}}(s,a)
\right]
$$

pero √∫nicamente permite updates $\theta$ tal que la divergencia sea peque√±a.


#### Ventajas de TRPO
- Evita que la pol√≠tica cambie demasiado r√°pido.  
- Mantiene un comportamiento m√°s estable que Policy Gradient vanilla.  
- Reduce el riesgo de colapsar la pol√≠tica hacia acciones malas.  

#### Desventaja
- **Computacionalmente costoso:**  
  Requiere resolver un problema de optimizaci√≥n con restricciones (m√©todo conjugado, hessianos aproximados, etc.).

Esto llev√≥ a desarrollar un m√©todo m√°s simple‚Ä¶

### **PPO ‚Äî Proximal Policy Optimization**

PPO es una versi√≥n pr√°ctica de TRPO: mantiene la idea de limitar cu√°nto puede cambiar la pol√≠tica, pero evita la optimizaci√≥n costosa basada en KL Divergence. En su lugar, usa un mecanismo simple de **clipping** para restringir el tama√±o del update.

---


#### Idea clave del m√©todo

Se calcula el **ratio** entre la pol√≠tica nueva y la antigua:

$$
r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}
$$

PPO fuerza este ratio a permanecer cerca de 1.  
El l√≠mite se define con un hiperpar√°metro $\epsilon$ (t√≠picamente $0.2$) y el objetivo que se maximiza es:

$$
L^{PPO}(\theta) =
\mathbb{E}\left[
\min\left(
r(\theta)A,\;
\text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)A
\right)
\right]
$$

donde:

- $r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}$
- $A$ es el Advantage estimado para esa muestra.
##### üîπ 1. Cuando el ratio est√° dentro del rango permitido

Si:

$$
1 - \epsilon \le r(\theta) \le 1 + \epsilon
$$

entonces el gradiente es el mismo que en Policy Gradient:

$$
\nabla_\theta L =
A \, r(\theta)\, \nabla_\theta \log \pi_\theta(a|s)
$$

que proviene de:

$$
\nabla_\theta r(\theta)
= r(\theta)\, \nabla_\theta \log \pi_\theta(a|s)
$$

Este caso permite actualizar la pol√≠tica normalmente.

---

##### üîπ 2. Cuando el ratio se sale del rango (clipping activado)

Si:

$$
r(\theta) < 1 - \epsilon 
\quad \text{o} \quad 
r(\theta) > 1 + \epsilon
$$

el objetivo usa la versi√≥n recortada:

$$
\text{clip}(r(\theta), 1-\epsilon, 1+\epsilon) A
$$

Esta expresi√≥n es **constante respecto a** $\theta$, por lo que:

$$
\nabla_\theta L = 0
$$

‚û° No se actualiza la pol√≠tica para esta transici√≥n.  
‚û° Se evita que el paso de actualizaci√≥n sea demasiado grande.


De esta forma, PPO evita que la pol√≠tica cambie demasiado r√°pido y se vuelva inestable.

---

#### Actualizaci√≥n Actor‚ÄìCritic en PPO

PPO sigue el esquema Actor‚ÄìCritic:

- **Critic ($V_\phi$)** estima $V(s)$ para construir la funci√≥n Advantage:  
  $$A = R(\tau) - V_\phi(s)$$

- **Actor ($\pi_\theta$)** actualiza sus par√°metros usando el objetivo clipped.

El gradiente real se calcula como:

$$
\nabla_\theta J(\pi_\theta) =
\mathbb{E}_\tau
\left[
\nabla_\theta \log \pi_\theta(a_t|s_t) \cdot A_t
\right]
$$

pero modulado por el clipping, es decir, s√≥lo se propaga si el ratio est√° dentro del rango permitido.

Esto produce:
- actualizaci√≥n para la acci√≥n elegida:  

$$
\nabla_\theta J(\pi_\theta)
= \mathbb{E}_{\tau \sim \pi_\theta}
\left[
\sum_{t=0}^{T}
\frac{s_i (1 - s_i)}{sOld_j} \cdot
\text{Adv}
\right]
$$

- actualizaci√≥n ligera o nula para las acciones no elegidas:  

$$
\nabla_\theta J(\pi_\theta)
= \mathbb{E}_{\tau \sim \pi_\theta}
\left[
\sum_{t=0}^{T}
\frac{s_i (s_j)}{sOld_j} \cdot
\text{Adv}
\right]
$$

seg√∫n si fueron penalizadas por el clipping.

En PPO, la actualizaci√≥n de par√°metros depende de c√≥mo cambian las probabilidades de la pol√≠tica nueva respecto a la antigua: 

> Para la acci√≥n elegida, el gradiente es fuerte: aumenta o disminuye su probabilidad seg√∫n el Advantage, y se escala por el ratio entre pol√≠tica nueva y vieja. Esto refuerza buenas acciones y penaliza malas, pero solo dentro de un l√≠mite seguro determinado por el clipping.

> Para las acciones no elegidas, la actualizaci√≥n es indirecta y mucho m√°s peque√±a: se ajustan sus probabilidades para mantener una distribuci√≥n coherente, pero sin alterar dr√°sticamente la pol√≠tica. Si el ratio sale del rango permitido, PPO aplica clipping y ambos tipos de actualizaciones se reducen o eliminan, evitando cambios bruscos o inestables.

**En esencia: la acci√≥n elegida recibe el update principal, las no elegidas solo peque√±os ajustes, y PPO asegura que nada cambie demasiado r√°pido.**

---

#### Ventajas de PPO
- Reduce varianza y evita actualizaciones peligrosas.
- Mantiene estabilidad similar a TRPO sin su costo computacional.
- Permite m√∫ltiples pasos de optimizaci√≥n por cada batch (a diferencia de PG cl√°sico).
- Es actualmente uno de los m√©todos est√°ndar en entornos de RL modernos.

---

### Comparaci√≥n PPO y TRPO
| M√©todo | Estabilidad | Coste computacional | Control del cambio |
|--------|-------------|---------------------|--------------------|
| Policy Gradient | Baja | Bajo | Ninguno |
| **TRPO** | Muy alta | **Muy alto** | KL Divergence estricta |
| **PPO** | Alta | Bajo/Medio | Ratio con clipping (trust region suave) |

---

## **Deep Q-Networks (DQN)**

Cuando el espacio de estados es demasiado grande para usar una tabla Q cl√°sica (como en videojuegos tipo Atari donde los estados son **im√°genes**), se utiliza una **Red Neuronal Convolucional (CNN)** para aproximar la funci√≥n de acci√≥n-valor:

$$
Q_\theta(s, a)
$$

### **Idea General**
- **Input:** Una imagen o stack de im√°genes (estado).
- **Output:** Un vector de valores Q, uno por cada acci√≥n posible.
- **Aprendizaje:** La red no aprende a ‚Äúclasificar‚Äù, sino a detectar **caracter√≠sticas visuales que indican valor futuro**.



## **¬øQu√© aprende realmente un DQN?**
Una CNN de visi√≥n tradicional aprende *features* para reconocer objetos.  
Un **DQN** aprende *features* que le dicen:

> "Si est√°s viendo este patr√≥n visual, esta acci√≥n futura tiende a generar buena recompensa."

Es decir, aprende a ver la pantalla como un humano experto: detecta **se√±ales √∫tiles para sobrevivir, esquivar, atacar, etc.**



### **El Ciclo Completo de Aprendizaje de un DQN**

---

### **1. Observaci√≥n del Estado**

El agente recibe el estado actual:

$$ s_t $$

que suele ser una imagen o un stack de varios frames (para capturar movimiento).

La red neuronal (CNN + capas densas) produce un **vector de valores Q** para cada acci√≥n:

$$
Q_\theta(s_t, a_1),\;
Q_\theta(s_t, a_2),\;
\dots,\;
Q_\theta(s_t, a_n)
$$

Cada valor representa la estimaci√≥n de cu√°n buena es cada acci√≥n desde ese estado.

---

### **2. Selecci√≥n de Acci√≥n (Exploraci√≥n vs. Explotaci√≥n)**

El agente decide la acci√≥n mediante una pol√≠tica **$\varepsilon$-greedy**:

- Con prob. $\varepsilon$: toma una **acci√≥n aleatoria** (explora).  
- Con prob. $1 - \varepsilon$: toma la **mejor acci√≥n seg√∫n la red**.

Formalmente:

$$
a_t = 
\begin{cases}
\text{acci√≥n aleatoria}, & \text{si } \text{Uniform}(0,1) < \varepsilon \\
\arg\max_a Q_\theta(s_t, a), & \text{si } \text{Uniform}(0,1) \ge \varepsilon
\end{cases}
$$

---

### **3. Ejecuci√≥n de la Acci√≥n**

Tras ejecutar $a_t$, el entorno devuelve:

- el siguiente estado $s_{t+1}$
- la recompensa inmediata $r_t$
- un indicador $done$ que dice si el episodio termin√≥

---

### **4. Almacenamiento en Replay Buffer**

Se guarda la transici√≥n completa:

$$
(s_t, a_t, r_t, s_{t+1}, done)
$$

El **Replay Buffer** permite:

- romper correlaciones temporales en los datos  
- entrenar la red con *mini-batches* independientes  
- reutilizar experiencias muchas veces

---

### **5. Muestreo de un Mini-Batch**

Para entrenar, se selecciona un conjunto aleatorio de experiencias:

$$
\{(s_i, a_i, r_i, s'_i, done_i)\}_{i=1}^N
$$

Esto da gradientes m√°s estables que entrenar con datos consecutivos del episodio.

---

### **6. C√°lculo del Target TD**

El objetivo (target) para el aprendizaje proviene del m√©todo de **Temporal Difference (TD)** y se calcula usando la **Target Network** $Q_{\theta^-}$, una copia estable de la red.

- Si la transici√≥n **no es terminal**:

$$
y_i = r_i + \gamma \max_{a'} Q_{\theta^-}(s'_i, a')
$$

- Si **es terminal**:

$$
y_i = r_i
$$

La red principal NUNCA se usa para el $\max$ aqu√≠; por estabilidad, solo se usa la Target Network.

---

### **7. Actualizaci√≥n de la Q-Network**

La red principal se entrena minimizando el error cuadr√°tico entre la predicci√≥n y el target:

$$
L(\theta) = 
\frac{1}{N} \sum_{i=1}^N 
\left( y_i - Q_\theta(s_i, a_i) \right)^2
$$

Actualizaci√≥n por gradiente descendente:

$$
\theta \leftarrow \theta - \alpha \nabla_\theta L(\theta)
$$

---

### **8. Actualizaci√≥n de la Target Network**

Cada cierto n√∫mero de pasos, sincronizamos las redes:

- **Hard update**:

$$
\theta^- \leftarrow \theta
$$

- **Soft update** (m√°s estable):

$$
\theta^- \leftarrow \tau \theta + (1 - \tau)\theta^-
$$

donde $0 < \tau \ll 1$ (ej. $10^{-3}$).

---


### **Resumen Conceptual**
**Un DQN:**
- Observa un estado visual.  
- Estima $Q(s, a)$ mediante una CNN.  
- Usa TD-learning para ajustar esos valores.  
- Aprende a reconocer patrones visuales que indican qu√© acciones son mejor a largo plazo.  
- Se entrena de manera estable gracias a **Replay Memory** y **Target Networks**.

---

## Model-Based & Advanced Architectures
El aprendizaje por refuerzo basado en modelos entrena un modelo a partir del muestreo de la din√°mica del entorno y entrena su pol√≠tica a partir del muestreo de este modelo. A continuaci√≥n se presentan estos modelos:

---

### **World Models**

Los *World Models* buscan que el agente no solo reaccione al entorno, sino que **aprenda su propia simulaci√≥n interna del mundo**. La gracia es que el agente deja de entrenar directamente sobre im√°genes crudas y empieza a entrenar en un espacio m√°s simple y estructurado. Este ‚Äúmundo interno‚Äù se construye usando tres m√≥dulos: un **VAE**, un **MDN-RNN** y un **Controller**.

#### **¬øQu√© es el Espacio Latente?**

Un **espacio latente** es una **versi√≥n comprimida de los datos originales** donde solo se conservan las caracter√≠sticas m√°s importantes.  
Este espacio permite representar informaci√≥n compleja en pocas dimensiones, facilitando el an√°lisis, la predicci√≥n y la generaci√≥n de datos.

En muchos modelos, este espacio se describe mediante los **par√°metros de varias distribuciones gaussianas** (medias y varianzas), lo que permite capturar estructuras complejas del mundo real en una forma compacta y manipulable.

Ejemplo: Una imagen de 64√ó64√ó3 son 12‚Äâ288 valores. El VAE puede convertir eso en un vector de, digamos, 32 dimensiones.

Lo importante:

- No es un ‚Äúpixelado‚Äù ni un recorte; es una **codificaci√≥n abstracta** de las caracter√≠sticas relevantes.  
- En un buen espacio latente, puntos cercanos representan estados visualmente o sem√°nticamente parecidos.  
- Aprender pol√≠ticas en este espacio es m√°s f√°cil porque el agente trabaja con una **versi√≥n organizada** del mundo en lugar de im√°genes ruidosas y enormes.

En resumen: el latente es ‚Äúlo que necesitas saber, sin la basura‚Äù.


#### **1. VAE (Vision Module)**

El **Variational Autoencoder** toma cada frame del entorno y lo comprime a un vector latente:

$$ z_t = \text{Encoder}(s_t) $$

Ese vector:

- contiene la informaci√≥n relevante de la imagen  
- elimina detalles irrelevantes  
- sigue una distribuci√≥n gaussiana aprendida

La estructura general es:

$$
s_t \rightarrow \text{Encoder} \rightarrow z_t
$$

y tambi√©n aprende a decodificar:

$$
z_t \rightarrow \text{Decoder} \rightarrow \hat{s}_t
$$

Esto obliga al VAE a aprender una representaci√≥n compacta y √∫til.


#### **2. MDN-RNN (Memory Module)**

Una vez que tenemos estados comprimidos $z_t$, el siguiente paso es aprender **c√≥mo evoluciona el mundo**.

Para eso, se usa un **Recurrent Neural Network** (t√≠picamente un LSTM), pero no cualquiera: se convierte en un **Mixture Density Network** (MDN).  Esto significa que la red no predice un √∫nico siguiente estado, sino los **par√°metros de varias distribuciones gaussianas**:

$$
P(z_{t+1} \mid z_t, a_t, h_t)
$$

donde $h_t$ es el hidden state del RNN.

¬øPor qu√© mezclar gaussianas?

- Porque el futuro no siempre es determinista.  
- En muchos juegos, desde un mismo estado pueden ocurrir varios eventos posibles.  
- El MDN captura esa **incertidumbre** al predecir m√∫ltiples gausianas (cada una con su media, varianza y peso).

Lo que hace el MDN-RNN:

- Modela las din√°micas del entorno en el espacio latente.  
- Predice si el episodio terminar√° pronto.  
- Mantiene una memoria $h_t$ que representa el ‚Äúestado interno‚Äù de la secuencia.

En f√≥rmula simplificada:

$$
(z_t, a_t, h_t) \rightarrow \text{MDN-RNN} \rightarrow (\text{Gaussians for } z_{t+1},\; \text{done probability})
$$

#### **3. Controller (Policy Module)**

Ahora que ya existe:

- una representaci√≥n visual comprimida $z_t$
- un modelo del futuro $h_t$

solo falta un m√≥dulo que tome decisiones.

El **Controller** suele ser una red neuronal muy peque√±a (incluso lineal en el paper original):

$$
a_t = C(z_t, h_t)
$$

Este m√≥dulo es el que implementa la pol√≠tica. Lo interesante:

- Ya no necesita ver im√°genes crudas.  
- Ya no necesita aprender las din√°micas del entorno.  
- Solo aprende a actuar usando el simulador interno que construyeron el VAE y el MDN-RNN.

Esto reduce brutalmente la complejidad del problema.

---

#### **La Gran Ventaja: El Agente Puede ‚ÄúSo√±ar‚Äù**

Como el MDN-RNN puede predecir $z_{t+1}$ y el indicador de finalizaci√≥n, el agente puede:

- **simular episodios completos dentro de su mente**,  
- sin tocar el entorno real,  
- generando millones de experiencias baratas y r√°pidas.

El Controller puede entrenarse completamente dentro de esta simulaci√≥n:

$$
\text{Controller} \;\text{entrena en el mundo generado por}\; (VAE + MDN\text{-}RNN)
$$

y luego se transfiere al entorno real.

---

#### **Pros y Contras**

**Pros:**
- Enorme eficiencia de muestras: la pol√≠tica se entrena en el espacio latente y en simulaciones internas.  
- Permite entrenar m√∫ltiples tareas sobre el mismo modelo del mundo.  
- El VAE reduce la complejidad del input visual de forma masiva.

**Contras:**
- El MDN-RNN a veces falla al modelar din√°micas dif√≠ciles.  
- El Controller puede aprender a explotar ‚Äúbugs‚Äù del mundo simulado. Luego esas pol√≠ticas no funcionan en el entorno real.  
- Requiere entrenar tres modelos separados (cierta complejidad).

---

### **Deep Planning Network (PlaNet)**

PlaNet es una mejora sobre **World Models**, dise√±ada para realizar *planeaci√≥n* directamente en el **espacio latente**, sin necesidad de entrenar una pol√≠tica tradicional.

#### üîπ **Idea Central**
En lugar de predecir im√°genes o entrenar un policy network, PlaNet:

1. **Aprende un modelo del mundo en espacio latente.**  
   El modelo predice:
   - el siguiente estado latente,
   - la recompensa futura,
   - y la probabilidad de terminar el episodio.

2. **Simula (‚Äúrollouts‚Äù) miles de trayectorias posibles dentro del modelo**, sin usar el entorno real.

3. **Elige la secuencia de acciones** que maximiza la suma de recompensas simuladas.

> Es decir: *planifica*, no solo *reacciona*.

#### üîπ ¬øC√≥mo funciona PlaNet?

##### **1. Aprendizaje del modelo din√°mico**
El sistema entrena un modelo en espacio latente que captura:
- transiciones latentes:  
  $$ z_{t+1} = f(z_t, a_t) $$
- recompensas:  
  $$ r_t = g(z_t, a_t) $$
- terminaci√≥n del episodio.

Este modelo NO trabaja con im√°genes directamente; utiliza una codificaci√≥n latente compacta.

##### **2. Rollouts imaginados**
Con el modelo entrenado, PlaNet **simula miles de futuros posibles**:

$$
(z_t, a_t) \rightarrow z_{t+1} \rightarrow z_{t+2} \rightarrow \dots
$$

Cada secuencia genera una recompensa acumulada:

$$
R = \sum_{k=0}^{H} r_{t+k}
$$

donde $H$ es el horizonte de planeaci√≥n.


##### **3. Optimizaci√≥n de la secuencia de acciones**
PlaNet usa m√©todos como **CEM (Cross-Entropy Method)** para buscar acciones que maximicen $R$.

No aprende una pol√≠tica expl√≠cita:  
> *elige acciones optimizando directamente la recompensa predicha*.

---
#### ‚úîÔ∏è **Pros**
- **>5000% m√°s eficiente en muestras**: casi no requiere interacci√≥n con el entorno real.
- Planea en un espacio comprimido ‚Üí es m√°s r√°pido y estable que trabajar con im√°genes.
- Puede reutilizar el mismo modelo para m√∫ltiples tareas (*multi-task*).

#### ‚úñÔ∏è **Contras**
- El **modelo del mundo puede fallar** al representar din√°micas complejas.
- El agente puede aprender a **explotar errores del modelo**, generando pol√≠ticas que no funcionan en el entorno real.
- Requiere c√≥mputo considerable para simular miles de trayectorias en cada paso.

#### **Resumen en una frase**
PlaNet no aprende una pol√≠tica:  
> **‚ÄúImagina‚Äù miles de futuros en su espacio latente, eval√∫a sus recompensas y act√∫a siguiendo el mejor plan.**

---

### **Curiosity Driven Exploration**

Cuando un entorno tiene **recompensas extremadamente escasas**, el agente puede pasar miles de episodios sin recibir se√±al √∫til.  
Para evitar que la pol√≠tica ‚Äúno aprenda nada‚Äù, se introduce un mecanismo interno de motivaci√≥n:

#### **üîπ Intrinsic Curiosity Module (ICM)**

El ICM genera una **recompensa intr√≠nseca (se√±al de motivaci√≥n generada internamente por el agente, no por el entorno, cuyo prop√≥sito es incentivar la exploraci√≥n)** que motiva al agente a explorar zonas donde el modelo a√∫n no sabe predecir bien.

El m√≥dulo tiene **dos componentes principales**:

##### **1. Inverse Model (IM)**  
Recibe dos estados consecutivos en *feature space*:

$$ \phi(s_t),\; \phi(s_{t+1}) $$

y predice qu√© acci√≥n los conect√≥:

$$ \hat{a}_t = IM(\phi(s_t), \phi(s_{t+1})) $$

Su p√©rdida es una **cross-entropy**:

$$
L_I = \text{CE}(a_t,\; \hat{a}_t)
$$

Sirve para aprender representaciones de estados √∫tiles y consistentes (evita triviales cambios de p√≠xel).

---

##### **2. Forward Model (FM)**  
Predice el siguiente estado latente usando el estado actual y la acci√≥n real:

$$
\hat{\phi}(s_{t+1}) = FM(\phi(s_t), a_t)
$$

La **intrinsic reward** proviene del error de esta predicci√≥n:

$$
r^{int}_t = \frac{1}{2} \left\|\, \hat{\phi}(s_{t+1}) - \phi(s_{t+1}) \,\right\|^2
$$

Zonas donde el modelo falla ‚Üí **zonas interesantes** ‚Üí el agente quiere visitarlas.

---

#### **3. Funci√≥n de P√©rdida Total del ICM (Intrinsic Curiosity Module)**

El ICM genera **recompensa intr√≠nseca** a partir de qu√© tan dif√≠cil es predecir las consecuencias de las propias acciones del agente.  
Para lograrlo, el m√≥dulo usa dos partes:

1. **Inverse Model (IM):**  
   Aprende a predecir la acci√≥n ejecutada $a_t$ a partir del par de estados codificados:  
   $$
   (\phi(s_t), \phi(s_{t+1}))
   $$  
   Esto obliga al codificador $\phi(\cdot)$ a retener √∫nicamente *informaci√≥n controlable por el agente*.

2. **Forward Model (FM):**  
   Predice la representaci√≥n futura:  
   $$\hat{\phi}(s_{t+1}) = F(\phi(s_t), a_t)
   $$  
   El error de esta predicci√≥n mide qu√© tan sorprendente o novedoso es el cambio en el entorno, y se usa como **recompensa intr√≠nseca**.

**P√©rdida total del ICM**

El ICM entrena ambos modelos mediante la funci√≥n:

$$
L_{ICM} = (1 - \beta) L_I + \beta L_F
$$

donde:

- $\beta$ controla el equilibrio entre aprender a **predecir acciones** (IM) y **predecir el futuro** (FM).
- $L_I$: p√©rdida del *inverse model*, generalmente entrop√≠a cruzada al predecir $a_t$.
- $L_F$: p√©rdida del *forward model* en espacio latente:

$$
L_F = \left\| \hat{\phi}(s_{t+1}) - \phi(s_{t+1}) \right\|^2
$$

---

#### **Recompensa intr√≠nseca como error de predicci√≥n**

La recompensa intr√≠nseca surge del error del forward model:

$$
r_t^{int} = \frac{1}{2}
\left\| 
\hat{\phi}(s_{t+1}) - \phi(s_{t+1}) 
\right\|^2
$$

Esta cantidad es alta cuando el agente encuentra **situaciones desconocidas o dif√≠ciles de predecir**, incentivando la exploraci√≥n.

---

#### **Recompensa total usada por la Pol√≠tica**

La pol√≠tica sigue optimizando una recompensa combinada:

$$
r_t^{total} = r_t^{extrinsic} + \lambda \, r_t^{int}
$$

donde:

- $r_t^{extrinsic}$ es la recompensa real del entorno,
- $r_t^{int}$ proviene del ICM,
- $\lambda$ controla cu√°nto "pesa" la curiosidad.

---

#### **Interpretaci√≥n completa**

Cuando el agente toma una acci√≥n $a_t$ en $s_t$ y pasa a $s_{t+1}$:

1. Ambos estados se codifican:  
   $$\phi(s_t),\; \phi(s_{t+1})$$

2. El **Inverse Model** aprende la acci√≥n que caus√≥ ese cambio ‚Üí hace que $\phi(\cdot)$ ignore *ruido o elementos incontrolables*.

3. El **Forward Model** predice $\hat{\phi}(s_{t+1})$ a partir de $\phi(s_t)$ y $a_t$.  
   Su error define la curiosidad.

4. La pol√≠tica maximiza la suma de recompensas extr√≠nsecas + intr√≠nsecas.

**Resultado:**  
El agente explora de forma robusta, sin distraerse con cambios aleatorios del entorno que **no est√°n afectados por sus acciones** (un beneficio clave del codificador $\phi$).

- Le da al agente **motivaci√≥n propia** para explorar, incluso cuando no hay recompensas externas.
- Act√∫a como **preentrenamiento de exploraci√≥n**: aprende la estructura del entorno.
- Funciona especialmente bien en **entornos con recompensas muy escasas o retrasadas**.

---

#### **Problemas y Limitaciones**

##### **1. Exploitation de ruido**  
Si hay una zona del entorno donde las observaciones cambian aleatoriamente, el Forward Model fallar√° siempre ‚Üí  
intrinsic reward muy alto ‚Üí el agente entra en un *loop* buscando solo esa zona.

##### **2. Aprendizajes no transferibles**  
El agente puede aprender a explorar bien, pero no necesariamente a optimizar la tarea si:
- el objetivo real est√° lejos,  
- el entorno tiene din√°micas enga√±osas.

##### **3. Coste adicional**  
Entrenar IM + FM + Policy agrega complejidad computacional.

---

#### **Resumen**

> **Curiosity Driven Exploration** permite que el agente encuentre comportamientos √∫tiles incluso sin recompensas externas.  
El ICM aprende qu√© partes del entorno son *dif√≠ciles de predecir* y usa ese error como una recompensa interna para impulsar la exploraci√≥n, aunque puede ser enga√±ado por entornos ruidosos.

---
### Meta-Reinforcement Learning (Meta-RL / Learning to Learn)

Meta-RL busca que un agente **aprenda a adaptarse r√°pido** cuando la tarea o las reglas del entorno cambian (p. ej. bandits con probabilidades cambiantes, metas que se mueven en un laberinto, variantes de un videojuego). En vez de aprender una sola pol√≠tica para un problema fijo, se aprende una **estrategia meta** que permite obtener buenas pol√≠ticas con muy pocos pasos de interacci√≥n en una nueva tarea.

> **¬øQu√© problema resuelve?**
En entornos no estacionarios o en familias de tareas (distribuci√≥n de tareas $p(\mathcal{T})$), los m√©todos RL tradicionales (model-free) requieren muchas interacciones para reaprender. Meta-RL intenta **aprender la estructura** entre tareas para que la adaptaci√≥n sea *r√°pida* (few-shot).

Se define una distribuci√≥n de tareas $p(\mathcal{T})$. Para cada tarea $\mathcal{T}$ tenemos una p√©rdida/funci√≥n de rendimiento $L_{\mathcal{T}}(\cdot)$. El objetivo meta es:

$$
\min_\theta \; \mathbb{E}_{\mathcal{T}\sim p(\mathcal{T})}\big[\, L_{\mathcal{T}}\big(U(\theta, \mathcal{D}_{\mathcal{T}})\big)\,\big]
$$

donde:
- $\theta$ son los par√°metros meta (inicializaci√≥n, arquitect., etc.).  
- $\mathcal{D}_{\mathcal{T}}$ son las pocas experiencias recolectadas en la tarea $\mathcal{T}$.  
- $U(\theta,\mathcal{D}_{\mathcal{T}})$ es la **regla de adaptaci√≥n** (por ejemplo una actualizaci√≥n de gradiente, o la evoluci√≥n del estado oculto en una RNN).

---

#### Estrategias comunes en Meta-RL

##### 1. **Optimization-based (p. ej. MAML for RL)**
- Aprender una **inicializaci√≥n** de par√°metros tal que unas pocas actualizaciones de gradiente en una nueva tarea producen una buena pol√≠tica.

##### 2. **Recurrent / Contextual policies (p. ej. RL¬≤, Prefrontal Network)**
- La pol√≠tica incluye memoria (LSTM/GRU/Transformer). En lugar de actualizar par√°metros con gradiente, la **memoria interna** (estado oculto) se actualiza autom√°ticamente con secuencia de $(s,a,r)$ y codifica el *contexto* / estructura de la tarea.
- Entrenamiento meta: expones la RNN a m√∫ltiples episodios por tarea; la RNN aprende a ‚Äúleer‚Äù se√±ales (acciones, recompensas anteriores) y adaptar comportamiento *on-the-fly*.

##### 3. **Model-based meta-RL**
- Aprender un **modelo de din√°mica** que sea compartible entre tareas y usarlo para planificar o para adaptar pol√≠tica r√°pidamente.
- Ejemplos: adaptar par√°metros del modelo con pocas muestras y planear en ese modelo adaptado.

---

#### **Arquitecturas Prefrontal Network**
- **Entrada:** adem√°s del estado actual $s_t$, se alimentan se√±ales de contexto: acci√≥n previa $a_{t-1}$, recompensa previa $r_{t-1}$, bandera de fin de episodio, otros indicadores.  
- **Red recurrente (LSTM/GRU):** mantiene un estado oculto $h_t$ que resume historia breve y permite inferir la tarea actual.  
- **Salida:** pol√≠tica $\pi(a|s,h)$ y/o critic $V(s,h)$.  
- El LSTM act√∫a como una *memoria de meta-aprendizaje* (simula la funci√≥n de la corteza prefrontal).

---

#### Ejemplo intuitivo: 2-armed bandit no estacionario
- Tarea: cada episodio, las probabilidades de los dos brazos pueden cambiar.
- Un agente meta-entrenado aprende a usar la secuencia de recompensas recientes para inferir cu√°l brazo es mejor (sin fine-tune), gracias a su memoria (LSTM) o a una inicializaci√≥n que se adapta r√°pido (MAML).

---

#### ¬øPor qu√© funciona? ‚Äî Intuici√≥n
- Muchas tareas comparten estructura (p. ej. ‚Äúhay dos tipos de din√°mica‚Äù, ‚Äúlas recompensas cambian lentamente‚Äù).  
- Meta-RL explota esas regularidades: aprende **c√≥mo aprender** ‚Äî reglas de actualizaci√≥n o pol√≠ticas recurrentes que codifican estrategias de exploraci√≥n y explotaci√≥n eficientes bajo incertidumbre.

---

### **Decision Transformer (DT)**  
Un enfoque moderno que reformula el *Reinforcement Learning* como un **problema de modelado de secuencias**, de manera similar a GPT o modelos tipo BERT.

En lugar de aprender valores, TD-errors o pol√≠ticas expl√≠citas, el Decision Transformer aprende a **predecir la siguiente acci√≥n** a partir de una secuencia pasada de:
- estados
- acciones
- recompensas
- *return-to-go* (suma futura de recompensas deseada)

DT aprende patrones de **trayectorias completas** usando *causal attention*.

---

#### **C√≥mo funciona**

##### **1. Representaci√≥n como secuencia**
Cada paso se convierte en tokens:

$$
(R_t, s_t, a_t), (R_{t+1}, s_{t+1}, a_{t+1}), \dots
$$

donde:

- $R_t$ = *return-to-go* (recompensa futura que queremos alcanzar)  
- $s_t$ = estado  
- $a_t$ = acci√≥n  

El modelo recibe varios de estos tokens concatenados como **una sola secuencia**, igual que una oraci√≥n en NLP.

---

##### **2. Modelo Transformer**
Como en GPT, utiliza *masked causal attention*:

- solo ve el pasado  
- predice el siguiente token: **la acci√≥n √≥ptima**  

El mecanismo de atenci√≥n permite:
- reconocer patrones largos en secuencias  
- ignorar partes irrelevantes del estado  
- aprender trayectorias buenas incluso cuando la mayor√≠a son sub√≥ptimas

---

##### **3. Predicci√≥n de la acci√≥n**
El transformador se entrena para resolver:

$$
(a_{t}) = f_{\text{Transformer}}(R_{t}, s_{t}, a_{t-1}, s_{t-1}, \dots)
$$

Es decir, aprende qu√© acci√≥n llevar√≠a al *return-to-go* deseado.

---

##### **4. Ventajas clave**
- No usa TD-learning ni una funci√≥n valor.  
- Maneja datos **offline**: aprende solo de secuencias grabadas.  
- Aprovecha *attention* para reconstruir pol√≠ticas de alto rendimiento aunque los datos vengan de comportamientos no √≥ptimos.  
- Escala muy bien con datos masivos, igual que los modelos de lenguaje.

---

##### **5. Intuici√≥n**
DT aprende:  
> *‚ÄúEn secuencias donde el objetivo era alto, la gente que hizo esto tom√≥ estas acciones‚Ä¶ as√≠ que yo tambi√©n las tomar√©.‚Äù*

---

## 10. RL from Human Feedback (RLHF)

Esquema para alinear IAs (como ChatGPT) con intenci√≥n humana:
1.  **Pretraining:** Supervised Learning (predicci√≥n de next-token).
2.  **SFT (Supervised Fine-Tuning):** Se ajusta con ejemplos de buenas preguntas/respuestas humanas.
3.  **Reward Modeling:** Se entrena una red neuronal para predecir una puntuaci√≥n (score) basada en rankings hechos por humanos (esto es mejor que aquello).
4.  **RL Optimization (PPO):** Se usa PPO para optimizar el modelo de lenguaje usando el Reward Model como fuente de recompensa.


---

# Information Theory 

## 1. ¬øQu√© es la Informaci√≥n?
La **informaci√≥n** se entiende como *variaciones en los datos*.  
Un **mensaje** es un conjunto de variaciones estructuradas siguiendo un patr√≥n.

El objetivo central de la teor√≠a de la informaci√≥n es:
> **Encontrar el mejor patr√≥n para transmitir mensajes por un canal ruidoso minimizando incertidumbre y p√©rdida.**

---

## 2. Transmitiendo un Mensaje en un Canal Ruidoso
Cuando enviamos un mensaje (por ejemplo, una palabra):

- El canal puede **corromper o perder** partes del mensaje.  
- El receptor debe pedir aclaraciones (‚Äú¬øqu√© letra era?‚Äù).
- Para minimizar preguntas, debemos dise√±ar un sistema eficiente para identificar el s√≠mbolo enviado.

---

## 3. Reduciendo al M√≠nimo las Preguntas (Bits)
La estrategia √≥ptima es codificar mensajes usando **dos s√≠mbolos**:  
`0` y `1` ‚Üí m√≠nima cantidad de estados.

Cada pregunta del receptor divide el espacio de posibilidades en dos:
- ‚Äú¬øEst√° en la primera mitad?‚Äù
- ‚Äú¬øEst√° en la segunda mitad?‚Äù

Ejemplo para letras A‚ÄìZ:
- 26 s√≠mbolos ‚Üí ¬øcu√°ntas preguntas m√≠nimas?  
- Resolver:  
  $$
  2^x = 26 \quad \Rightarrow \quad x = \log_2 26 ‚âà 4.7 \text{ bits}
  $$

Ejemplo para un mazo de 52 cartas:
$$
x = \log_2 52 ‚âà 5.7 \text{ bits}
$$

---

## 4. Informaci√≥n Total de un Mensaje

La **informaci√≥n total** de un mensaje depende de:

- $n$: n√∫mero de s√≠mbolos a transmitir  
- $s$: n√∫mero de s√≠mbolos posibles (tama√±o del alfabeto o conjunto de opciones)

La f√≥rmula para calcular la informaci√≥n total en **bits** es:

$$
I = n \cdot \log_2(s)
$$

### Ejemplos:

1. **Transmisi√≥n de letras:**
   - Alfabeto de 26 letras
   - Mensaje de 6 letras
   - Cada letra requiere en promedio:
     
     $$
     \log_2(26) \approx 4.7 \text{ bits}
     $$
     
   - Informaci√≥n total del mensaje:
     
     $$
     I = 6 \cdot 4.7 \approx 28.2 \text{ bits}
     $$

2. **Transmisi√≥n de cartas de un mazo:**
   - Mazo de 52 cartas
   - Mensaje de 5 cartas
   - Cada carta requiere en promedio:
     
     $$
     \log_2(52) \approx 5.7 \text{ bits}
     $$
     
   - Informaci√≥n total del mensaje:
     
     $$
     I = 5 \cdot 5.7 \approx 28.5 \text{ bits}
     $$

> **Interpretaci√≥n:** Cuantos m√°s s√≠mbolos posibles tenga el conjunto ($s$), m√°s bits se necesitan para transmitir cada s√≠mbolo. Esta medida refleja la **incertidumbre** de cada mensaje antes de ser transmitido.


---

## 5. Informaci√≥n con Probabilidades Desiguales

Cuando los s√≠mbolos no son igualmente probables, debemos calcular la **informaci√≥n promedio** usando **valores esperados**.

---

### Ejemplo: 4 s√≠mbolos con probabilidades distintas

Supongamos:

$$
P(A) = 0.5, \quad P(B) = 0.125, \quad P(C) = 0.125, \quad P(D) = 0.25
$$

Para identificar un s√≠mbolo, el n√∫mero de preguntas se ajusta seg√∫n la probabilidad:

1. Primero se pregunta si es $A$:  
   - Probabilidad = 0.5 ‚Üí **1 pregunta**
2. Luego $D$:  
   - Probabilidad = 0.25 ‚Üí **2 preguntas**
3. Finalmente $B$ y $C$:  
   - Probabilidad = 0.125 cada uno ‚Üí **3 preguntas**  

---

### C√°lculo del n√∫mero esperado de preguntas

$$
\#\text{questions} = P(A) \cdot 1 + P(D) \cdot 2 + P(B) \cdot 3 + P(C) \cdot 3
$$

$$
\#\text{questions} = 0.5 \cdot 1 + 0.25 \cdot 2 + 0.125 \cdot 3 + 0.125 \cdot 3 = 1.75
$$

> Esto representa el **n√∫mero promedio de preguntas** necesarias para identificar un s√≠mbolo en este conjunto no uniforme.

---

### Entrop√≠a

La **entrop√≠a** $H$ mide la incertidumbre promedio de la distribuci√≥n:

$$
H = - \sum_i p_i \log_2(p_i)
$$

Aplicando al ejemplo:

$$
H = - [0.5 \log_2 0.5 + 0.25 \log_2 0.25 + 0.125 \log_2 0.125 + 0.125 \log_2 0.125] \approx 1.75 \text{ bits}
$$

**Interpretaci√≥n:**

- Mayor probabilidad de un s√≠mbolo ‚Üí menos preguntas necesarias.  
- Menor entrop√≠a $H$ ‚Üí menor incertidumbre del mensaje.  
- La entrop√≠a refleja **la cantidad promedio de informaci√≥n** necesaria para transmitir un mensaje considerando probabilidades desiguales.


---

## 6. Entrop√≠a
La **entrop√≠a** mide cu√°nta incertidumbre existe en una distribuci√≥n:

$$
H = - \sum_{i} p_i \log_2 p_i
$$

Interpretaci√≥n:
- **H alta** ‚Üí alta incertidumbre, s√≠mbolos equiprobables.  
- **H baja** ‚Üí hay s√≠mbolos mucho m√°s probables que otros.

Ejemplos:
- Distribuci√≥n de sexo 30‚Äì34 a√±os: $H = 0.99$
- Distribuci√≥n en militares (91% hombres): $H = 0.43$

---

## 7. Information Gain (Ganancia de Informaci√≥n)
Es la reducci√≥n de entrop√≠a al conocer un atributo:

$$
IG = H(D) - H(D|a)
$$

Ejemplo:
$$
0.99 - 0.94 = 0.05
$$

Muy utilizado en:
- √Årboles de decisi√≥n
- Selecci√≥n de atributos

---

## 8. Divergencia KL (Kullback‚ÄìLeibler)
Mide cu√°n diferente es una distribuci√≥n **P** de una distribuci√≥n **Q**:

$$
KL(P‚ÄñQ) = \sum_i P(x_i) \log \frac{P(x_i)}{Q(x_i)}
$$

Propiedad clave:
- **No es sim√©trica:**  
  $$
  KL(P‚ÄñQ) \neq KL(Q‚ÄñP)
  $$

---

## 9. Cross-Entropy (P√©rdida en Redes Neuronales)
Usada en clasificaci√≥n y modelos probabil√≠sticos, otro m√©todo para calcular la diferencia entre dos distribuciones:

$$
H(P,Q) = - \sum_i P(x_i)\log Q(x_i)
$$

Interpretaci√≥n:
- P = la *verdad* (dataset)
- Q = nuestro *modelo*
- Cross-Entropy dice cu√°n mal Q aproxima a P.

---

## 10. Relaci√≥n entre Verdad, Datos y Modelo
En Machine Learning:

$$
P(modelo) \approx P(datos) \approx P(verdad)
$$

- La **verdad** es incognoscible.  
- Los **datos** son nuestro proxy.  
- El **modelo** intenta aproximarlos.

---


