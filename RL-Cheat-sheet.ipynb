{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a317f29",
   "metadata": {},
   "source": [
    "\n",
    "## RL Cheat Sheet: Tipos de Acción y Algoritmos\n",
    "\n",
    "| Tipo de Acción / PDF        | Algoritmo                  | Actualización Online / Fórmulas |\n",
    "|------------------------------|---------------------------|--------------------------------|\n",
    "| **Discreta (pocas acciones)** | Q-Learning (tabular / Deep Q) | **Simple:**<br>$$Q_k = Q_{k-1} + \\alpha (r_k - Q_{k-1})$$<br>**Bellman / TD:**<br>$$Q(s,a) \\gets Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$ |\n",
    "| **Discreta (política suave)** | Policy Gradient (Softmax) | **Acción tomada $i$:**<br>$$\\theta_i \\gets \\theta_i + \\alpha (1 - p_i)(r - b)$$<br>**Acciones no tomadas $j\\neq i$:**<br>$$\\theta_j \\gets \\theta_j - \\alpha p_j (r - b)$$ |\n",
    "| **Continua (acciones reales)** | Policy Gradient (Gaussian / REINFORCE) | **Media incremental:**<br>$$\\hat{\\mu}_{t+1} = \\hat{\\mu}_t + \\frac{1}{N}(x_{t+1} - \\hat{\\mu}_t)$$<br>**Varianza incremental:**<br>$$\\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{(x_{t+1} - \\hat{\\mu}_t)^2 - \\hat{\\sigma}^2_t}{t+1}$$<br>o alternativamente:<br>$$\\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{1}{N}((x_t - \\hat{\\mu}) - \\hat{\\sigma}^2_t)$$ |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Concepto | Fórmula / Descripción |\n",
    "|----------|--------------------|\n",
    "| **Media secuencial** | $$\\hat{\\mu}_{t+1} = \\hat{\\mu}_t + \\frac{(x_{t+1} - \\hat{\\mu}_t)}{N}$$ |\n",
    "| **Media normal (MLE)** | $$\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{N} \\sum_{i=1}^{N} x_i$$ |\n",
    "| **Varianza secuencial** | $$\\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{(x_t - \\hat{\\mu})^2 - \\hat{\\sigma}^2_t}{N}$$ |\n",
    "| **Varianza normal (MLE)** | $$\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{\\mu})^2$$ |\n",
    "| **Softmax** | $$\\pi(a=j \\mid s) = \\frac{e^{\\beta a_j}}{\\sum_{i=1}^{k} e^{\\beta a_i}}$$ |\n",
    "| **Derivada softmax (acción elegida)** | $$\\theta_i \\gets \\theta_i + \\alpha (1 - p_i)(r - b)$$ $$\\frac{\\partial S_j}{\\partial a_j} = S_j(1 - S_j)$$|\n",
    "| **Derivada softmax (acción no elegida)** | $$\\theta_j \\gets \\theta_j - \\alpha p_j (r - b)$$ $$\\frac{\\partial S_j}{\\partial a_i} = -S_j S_i$$ |\n",
    "| **Actualización parámetros (datos discretos)** | $$a_j \\gets a_j + \\alpha (1 - s_j) R$$ <br> $$a_i \\gets a_i + \\alpha (0 - s_i) R, \\quad i \\neq j$$ <br> $s_j$ = probabilidad actual de la acción $j$ |\n",
    "| **Q-Learning** | $$Q(s,a) \\gets Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$ $\\gamma$: factor de descuento → cuánto valen las recompensas futuras. <br> - Si $\\gamma \\approx 1$: agente paciente, valora recompensas futuras. <br> - Si $\\gamma$ es bajo: agente solo considera recompensas inmediatas. |\n",
    "| **Decaimiento exponencial de ε** | $$\\text{eps\\_threshold} = \\text{eps\\_end} + (\\text{eps\\_start} - \\text{eps\\_end}) \\cdot \\exp(- \\text{steps} / \\text{eps\\_decay})$$ <br> - Con probabilidad $1-\\epsilon$: explota → elige acción con mayor $Q$. <br> - Con probabilidad $\\epsilon$: explora → elige acción aleatoria. |\n",
    "| **PDF de distribución normal** | $$f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{- \\frac{(x - \\mu)^2}{2\\sigma^2}}$$ |\n",
    "| **Valor esperado** | $$E[f(X)] = \\sum p(x_i)\\, f(x_i)$$ $$Q_k = Q_{k-1} + \\frac{1}{k}(r_k - Q_{k-1})$$ |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76651fb3",
   "metadata": {},
   "source": [
    "## Ejemplo: Actualización de parámetros softmax (categorica) (Policy Gradient online)\n",
    "\n",
    "Condiciones iniciales:\n",
    "\n",
    "- Parámetros iniciales: \n",
    "$$\n",
    "\\theta = [0, 0]\n",
    "$$\n",
    "- Softmax inicial: \n",
    "$$\n",
    "\\text{softmax}(\\theta) = [0.5, 0.5]\n",
    "$$\n",
    "- Learning rate: $\\alpha = 0.1$  \n",
    "- Recompensa: $r = 1$  \n",
    "- Acción seleccionada: índice 0  \n",
    "\n",
    "Fórmula de actualización:\n",
    "\n",
    "$$\n",
    "\\theta \\gets \\theta + \\alpha \\cdot (\\text{grad} - \\text{probs}) \\cdot r\n",
    "$$\n",
    "\n",
    "donde para la acción seleccionada:  \n",
    "$$\n",
    "\\text{grad} = [1,0]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteraciones (resumen)\n",
    "\n",
    "| Iteración | $\\theta_0$ | $\\theta_1$ | Softmax $\\text{probs}$ |\n",
    "|-----------|------------|------------|-----------------------|\n",
    "| 0         | 0          | 0          | [0.5, 0.5]           |\n",
    "| 1         | 0.05       | -0.05      | [0.512, 0.488]       |\n",
    "| 2         | 0.0975     | -0.0975    | [0.524, 0.476]       |\n",
    "| 3         | 0.1426     | -0.1426    | [0.537, 0.463]       |\n",
    "| 4         | 0.1891     | -0.1891    | [0.549, 0.451]       |\n",
    "| 5         | 0.2344     | -0.2344    | [0.562, 0.438]       |\n",
    "\n",
    "- Los parámetros de la **acción elegida aumentan**, los de la **no elegida disminuyen**.  \n",
    "- Las probabilidades softmax favorecen progresivamente la acción seleccionada.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d77980",
   "metadata": {},
   "source": [
    "## Ejemplo: Actualización de parámetros con Policy Gradient (Continua) (Gaussian PDF, Online)\n",
    "\n",
    "### Condiciones iniciales\n",
    "\n",
    "- Distribución de acciones: $a \\sim \\mathcal{N}(\\mu, \\sigma^2)$  \n",
    "- Parámetros iniciales: \n",
    "$$\n",
    "\\mu_0 = 0, \\quad \\sigma_0^{2} = 1\n",
    "$$\n",
    "- Learning rate implícito mediante promedio online: $\\frac{1}{N}$  \n",
    "- Recompensas asumidas $r_t = 1$  \n",
    "- Acciones tomadas por iteración: $a = [0.5, 0.6, 0.4, 0.7, 0.3]$\n",
    "\n",
    "---\n",
    "\n",
    "### Fórmulas de actualización online\n",
    "\n",
    "- Media incremental:\n",
    "$$\n",
    "\\hat{\\mu}_{t+1} = \\hat{\\mu}_t + \\frac{1}{t+1} \\left(a_{t+1} - \\hat{\\mu}_t\\right)\n",
    "$$\n",
    "\n",
    "- Varianza incremental:\n",
    "$$\n",
    "\\hat{\\sigma}^2_{t+1} = \\hat{\\sigma}^2_t + \\frac{(a_{t+1} - \\hat{\\mu}_t)^2 - \\hat{\\sigma}^2_t}{t+1}\n",
    "$$\n",
    "\n",
    "> Nota: Esta es la forma **online**, equivalente a un promedio ponderado secuencial que se puede usar en Policy Gradient.\n",
    "\n",
    "---\n",
    "\n",
    "### Iteración 0\n",
    "\n",
    "- Acción: $a_1 = 0.5$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_1 = 0 + \\frac{0.5 - 0}{1} = 0.5\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_1^2 = 1 + \\frac{(0.5 - 0)^2 - 1}{1} = 0.25\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteración 1\n",
    "\n",
    "- Acción: $a_2 = 0.6$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_2 = 0.5 + \\frac{0.6 - 0.5}{2} = 0.55\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_2^2 = 0.25 + \\frac{(0.6 - 0.5)^2 - 0.25}{2} = 0.125\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteración 2\n",
    "\n",
    "- Acción: $a_3 = 0.4$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_3 = 0.55 + \\frac{0.4 - 0.55}{3} \\approx 0.5\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_3^2 = 0.125 + \\frac{(0.4 - 0.55)^2 - 0.125}{3} \\approx 0.0833\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteración 3\n",
    "\n",
    "- Acción: $a_4 = 0.7$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_4 = 0.5 + \\frac{0.7 - 0.5}{4} = 0.55\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_4^2 = 0.0833 + \\frac{(0.7 - 0.5)^2 - 0.0833}{4} \\approx 0.0917\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteración 4\n",
    "\n",
    "- Acción: $a_5 = 0.3$  \n",
    "- Media:\n",
    "$$\n",
    "\\mu_5 = 0.55 + \\frac{0.3 - 0.55}{5} = 0.5\n",
    "$$\n",
    "- Varianza:\n",
    "$$\n",
    "\\sigma_5^2 = 0.0917 + \\frac{(0.3 - 0.55)^2 - 0.0917}{5} \\approx 0.0834\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen de iteraciones\n",
    "\n",
    "| Iteración | Acción $a_t$ | $\\mu_t$ | $\\sigma^2_t$ |\n",
    "|-----------|--------------|---------|-------------|\n",
    "| 0         | 0.5          | 0.5     | 0.25        |\n",
    "| 1         | 0.6          | 0.55    | 0.125       |\n",
    "| 2         | 0.4          | 0.5     | 0.0833      |\n",
    "| 3         | 0.7          | 0.55    | 0.0917      |\n",
    "| 4         | 0.3          | 0.5     | 0.0834      |\n",
    "\n",
    "- La **media $\\mu$** se ajusta hacia las acciones tomadas con mayor recompensa.  \n",
    "- La **varianza $\\sigma^2$** refleja la dispersión de las acciones y se ajusta secuencialmente en cada iteración.  \n",
    "- Este enfoque es equivalente a una actualización online de Policy Gradient para distribuciones gaussianas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3cdbf8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejemplo: Actualización de Q-Learning (Tabular)\n",
    "\n",
    "##### Condiciones iniciales\n",
    "\n",
    "- Estado único: $s$  \n",
    "- Acciones posibles: $A = \\{a_0, a_1\\}$  \n",
    "- Valores iniciales: \n",
    "$$\n",
    "Q(s, a_0) = 0, \\quad Q(s, a_1) = 0\n",
    "$$\n",
    "- Learning rate: $\\alpha = 0.1$  \n",
    "- Recompensas recibidas por acción: $r = [1, 0.5, 1, 0, 0.7]$  \n",
    "- No se considera futuro (TD simple, $\\gamma = 0$)  \n",
    "\n",
    "##### Fórmula de actualización simple (Tabular Q-Learning)\n",
    "\n",
    "$$\n",
    "Q_k = Q_{k-1} + \\alpha \\cdot (r_k - Q_{k-1})\n",
    "$$\n",
    "\n",
    "> Nota: El nuevo Q es el viejo Q más una fracción del error (diferencia entre recompensa recibida y la predicción previa).\n",
    "\n",
    "---\n",
    "\n",
    "##### Iteraciones\n",
    "\n",
    "| Iteración | Acción tomada | $Q_{prev}$ | Recompensa $r$ | $Q_{new}$ |\n",
    "|-----------|---------------|------------|----------------|-----------|\n",
    "| 0         | $a_0$         | 0          | 1              | $0 + 0.1*(1-0) = 0.1$ |\n",
    "| 1         | $a_1$         | 0          | 0.5            | $0 + 0.1*(0.5-0) = 0.05$ |\n",
    "| 2         | $a_0$         | 0.1        | 1              | $0.1 + 0.1*(1-0.1) = 0.19$ |\n",
    "| 3         | $a_1$         | 0.05       | 0              | $0.05 + 0.1*(0-0.05) = 0.045$ |\n",
    "| 4         | $a_0$         | 0.19       | 0.7            | $0.19 + 0.1*(0.7-0.19) = 0.241$ |\n",
    "\n",
    "---\n",
    "\n",
    "##### Resumen\n",
    "\n",
    "- Los valores $Q(s, a)$ se **acercan progresivamente a la recompensa promedio** de cada acción.  \n",
    "- La acción $a_0$ tiene un Q mayor porque históricamente dio más recompensa.  \n",
    "- La **fórmula es muy intuitiva:** ajusta el valor anterior usando el error entre lo que se recibió y lo esperado.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
